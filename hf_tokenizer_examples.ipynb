{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d26e0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b53fb2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='pretrained/google/electra-small-discriminator', vocab_size=30522, model_max_len=10, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "model_input_names=['input_ids', 'token_type_ids', 'attention_mask']\n",
      "\n",
      "CPU times: user 31.2 ms, sys: 0 ns, total: 31.2 ms\n",
      "Wall time: 26.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pretrained_dir = \"pretrained/google/electra-small-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, model_max_length=10)\n",
    "print(f\"\"\"{repr(tokenizer)}\n",
    "model_input_names={repr(tokenizer.model_input_names)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931a64e",
   "metadata": {},
   "source": [
    "# Truncation problems\n",
    "- Truncate question or passage?\n",
    "- If passage is truncated, then answer span can also be truncated. How to get indexes of partial answer span over multiple passage chunks?\n",
    "- [`BatchEncoding.char_to_token`](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.char_to_token) does not work well with truncation. If original string is truncated, it cannot be mapped to token space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2243750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2028, 2048, 2093, 2028, 2048, 2093, 102, 2028, 102], [101, 2028, 2048, 2093, 2028, 2048, 2093, 102, 2028, 102], [101, 2028, 2048, 2093, 2028, 2048, 2093, 102, 2028, 102], [101, 6970, 3207, 11837, 4181, 3401, 2028, 102, 2028, 102], [101, 2028, 4654, 9006, 23041, 21261, 2048, 102, 2028, 102], [101, 2028, 2048, 4654, 9006, 23041, 21261, 102, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "contexts = [\n",
    "    \"one two three one two three\",\n",
    "    \"one two three one two three\",\n",
    "    \"one two three one two three\",\n",
    "    \"interdependence one\",\n",
    "    \"one excommunication two\",\n",
    "    \"one two excommunication\",\n",
    "]\n",
    "questions = [\"one\"] * len(contexts)\n",
    "answer_start = [0, 14, 0, 0, 4, 8]\n",
    "# must be the index of last char in the answer span!\n",
    "answer_end = [2, 16, 12, 14, 18, 22]  \n",
    "es = tokenizer(contexts, questions, truncation=False, padding=False)\n",
    "print(repr(es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3261128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=1, k=2, _ids=[2028], a=one\n",
      "j=4, k=5, _ids=[2028], a=one\n",
      "j=1, k=4, _ids=[2028, 2048, 2093], a=one two three\n",
      "j=1, k=6, _ids=[6970, 3207, 11837, 4181, 3401], a=interdependence\n",
      "j=2, k=6, _ids=[4654, 9006, 23041, 21261], a=excommunication\n",
      "j=3, k=7, _ids=[4654, 9006, 23041, 21261], a=excommunication\n"
     ]
    }
   ],
   "source": [
    "input_ids = es[\"input_ids\"]\n",
    "for i in range(len(answer_start)):\n",
    "    j = es.char_to_token(i, answer_start[i])\n",
    "    k = es.char_to_token(i, answer_end[i]) + 1\n",
    "    _ids = input_ids[i][j:k]\n",
    "    a = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(_ids))\n",
    "    print(f\"j={j}, k={k}, _ids={_ids}, a={a}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
