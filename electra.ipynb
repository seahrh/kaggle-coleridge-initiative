{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c7c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
    "from tqdm import tqdm\n",
    "import scml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6602a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()\n",
    "scml.seed_everything()\n",
    "seed = 31\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7c08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='pretrained/google/electra-small-discriminator', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "pretrained_dir = \"pretrained/google/electra-small-discriminator\"\n",
    "model_max_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, model_max_length=model_max_length)\n",
    "print(f\"{repr(tokenizer)}\\n{tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6ade8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained/google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at pretrained/google/electra-small-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"pretrained/google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(pretrained_dir)\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1aee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360000 entries, 0 to 359999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   Id                    360000 non-null  object\n",
      " 1   is_multi              360000 non-null  int8  \n",
      " 2   ground_truth          360000 non-null  object\n",
      " 3   dataset_labels        360000 non-null  object\n",
      " 4   is_impossible         360000 non-null  int8  \n",
      " 5   answer_start          360000 non-null  int16 \n",
      " 6   answer_end            360000 non-null  int16 \n",
      " 7   context               360000 non-null  object\n",
      " 8   context_token_length  360000 non-null  int16 \n",
      "dtypes: int16(3), int8(2), object(4)\n",
      "memory usage: 13.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet(\"input/train.parquet\")\n",
    "#train = train.sample(frac=0.07)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34457e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "len=360000\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"what dataset\"\n",
    "questions = [question] * len(train)\n",
    "enc = tokenizer(list(train[\"context\"]), questions, padding=\"max_length\")\n",
    "print(f\"{repr(enc.keys())}\\nlen={len(enc['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02a2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answer_start, answer_end, ids, is_impossible):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(is_impossible)):\n",
    "        j, k = 0, 0\n",
    "        if is_impossible[i] == 0:\n",
    "            j = encodings.char_to_token(i, answer_start[i])\n",
    "            if j is None:\n",
    "                #offsets = encodings[\"offset_mapping\"][i]\n",
    "                _id = ids[i]\n",
    "                raise ValueError(f\"start pos must not be None. i={i}, id={_id}, answer_start={answer_start[i]}\")  #offsets={offsets}\") \n",
    "            k = encodings.char_to_token(i, answer_end[i] - 1)\n",
    "            if k is None:\n",
    "                raise ValueError(\"end pos must not be None\")\n",
    "            if j > k:\n",
    "                raise ValueError(\"start pos must be less than or equals end pos\")\n",
    "        start_positions.append(j)\n",
    "        end_positions.append(k)\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f7847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3170c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 650 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "add_token_positions(\n",
    "    enc, \n",
    "    answer_start=list(train[\"answer_start\"]), \n",
    "    answer_end=list(train[\"answer_end\"]),\n",
    "    ids=list(train[\"Id\"]),\n",
    "    is_impossible=list(train[\"is_impossible\"]),\n",
    ")\n",
    "train_ds = MyDataset(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef07e6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del enc, questions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e188aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(repr(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813f9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22500/22500 [2:39:09<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=5.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "#lr=5e-4\n",
    "optim = AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    loss_mean = 0\n",
    "    steps = len(train_loader)\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss_mean += loss / steps\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch={epoch}, loss={loss_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cefc2e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForQuestionAnswering(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70552291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9916f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output\\\\tokenizer_config.json',\n",
       " 'output\\\\special_tokens_map.json',\n",
       " 'output\\\\vocab.txt',\n",
       " 'output\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20ffc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"output\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"output\")\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d3247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.sample(30)\n",
    "questions = [question] * 30\n",
    "contexts = list(df[\"context\"])\n",
    "golds = list(df[\"dataset_labels\"])\n",
    "is_impossible = list(df[\"is_impossible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c6b6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=mats could be found on eelgrass beds and bare substrate without distinguishable spectral variation, creating a broader range of spectral signatures found within the bay. Furthermore, there are two species of eelgrass that thrive in Willapa Bay, each with a unique signature. North American eelgrass (Zostera Marina) is larger leafed than Asian eelgrass (Zostera Japonica) and tends to occupy the inter-tidal zone (Backman, 1991;Phillips, 1984). The lighter hue of Asian eelgrass could easily be detected on the tidal flats, but could not be clearly distinguished from North American eelgrass in the inter-tidal zone. The combination of the two eelgrass species in the inter-tidal zone provided a unique spectral signature of its own, leading to three different signatures of eelgrass. There were other unidentifiable spectral signatures for which no source could be located. Several regions in the northern end of the study area had received sediment deposition following the flight mission and consequently no source for reflectance could be determined. The use of preliminary data gathered to measure spectral signatures was critical for the accurate interpretation of the Willapa Bay eelgrass photo set and will prove to be an integral component to future eelgrass distribution studies in Willapa Bay. Determining the size of the minimum mapping unit for this study proved to be problematic due to the presence of a non-native rush, Spartina spp., throughout Willapa Bay. C-CAP guidelines suggest that a minimum mapping unit of .03 hectares on the ground is appropriate for SRV distribution studies under ideal conditions. However, the prolific growth of Spartina spp., throughout Willapa Bay created a situation where countless island polygons of Spartina spp., clones would need to be excluded from eelgrass beds if the .03 hectare minimum mapping unit was used. To avoid mapping Spartina spp. rather than eelgrass, the minimum mapping unit size was increased to 1/10\" or 2.54 millimeters on the\n",
      "i=0, j=357, k=109\n",
      "a=IMPOSSIBLE\n",
      "g=Coastal Change Analysis Program|NOAA C-CAP\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=y in Moses Bayou is also impaired from bacteria, dioxin in edible tissue, and PCBs in edible tissue. Total maximum daily loads (TMDLs) in Moses Lake and Bayou for dioxin in edible tissue and PCBs in edible tissue are planned. Additional data is being collected before a management strategy is selected for the bacteria impairment.\n",
      "There are two major tidal inlets into Corpus Christi Bay. Aransas Pass (Corpus Christi Ship Channel), between Mustang Island and San Jose Island, which accounts for the majority of the tidal exchange between the bay and the Gulf of Mexico. Packery Channel, between the southwestern end of Mustang Island and North Padre Island, is manmade inlet that supplies a lesser amount of the bay's tidal exchange. Overall, the natural depth of the bay is relatively shallow, with an average depth of approximately 9 feet. Tides in Corpus Christi Bay under normal conditions are very small in amplitude, usually less than 3 feet between low and high tide. Wind speed and direction within Corpus Christi Bay plays an important role in affecting tide elevation. It can dampen or enhance the height of waves as well as their potential energy. Prevailing winds are from the southeast, with occasional strong northerly winds that are associated with passing cold fronts. Winds combined with seasonal tide events can greatly exacerbate the tidal range as well as move the range up or down by 1 or 2 feet. Storm tides during Category 4 or 5 hurricanes could be as high as 15-20 feet above normal water levels according to NOAA's Sea, Lake, and Overland Surge from Hurricanes (SLOSH) Model. The project site is located along Indian Point in Upper Corpus Christi Bay. Indian Point is a small peninsula that extends from the northeastern shore and extends towards Rincon Point on the southwestern shore. The two peninsulas separate Nueces Bay from Upper Corpus Christi Bay. Conditions within project area are primarily influenced by Corpus Christi Bay. The hydrology of the area is affected\n",
      "i=1, j=0, k=86\n",
      "a=IMPOSSIBLE\n",
      "g=SLOSH model\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=IBTrACS data for any data source that reported both MSW and MCP. While this fitting method deviates from recommendations by KZ07 regarding how the fit should be performed (i.e., observations should be binned and then fit), the focus here is on interagency and interannual differences rather than the absolute accuracy of any one formula. Such an analysis shows how operational procedures might have changed over time. The WPR parameters are shown in Fig. 3 as a time series along with the root-mean-square (RMS) error of the empirical fit and V 920, the wind speed corresponding to MCP 5 920 hPa. The RMS values provide insights into how much an agency followed any WPR. For instance, AR ended in 1987, thus forcing agencies to estimate intensity from satellites more often, as is done in Dvorak (1984) . The result is that after 1987, the maximum RMS is small (about 6 kt) because both wind and pressure were derived from the same satellite estimate. Conversely, RMS values in the early record (e.g., before 1970) show RMS values exceeding 15 kt, implying a consistent WPR was not routinely used to constrain wind to pressure or vice versa. It is of note that the RMS rarely shows a step change in any time series. The RMSs for TD-9635 and CMA gradually drift from more than 10 kt to near 5 kt in the 1980s. An exception is the JTWC switch to a different WPR in 2007.\n",
      "The impact of any change in procedures can be seen in the V 920 time series (Fig. 3, bottom) . Given the small bias compared to AR pressure data, any change in this value implies that there is likely a temporal bias in the reported wind speeds that could preclude climatic analysis (i.e., direct comparisons between years with vastly different V 920 ). The CMA and TD-9635 show a drift from 140-160 kt in the early record to 120 kt in the 1970s. It is likely a change in operational practice caused this gradual change. Given the previous validation of MCP from CMA (cf. Fig. 2) , it is possible that winds for CMA before 1970 are \n",
      "i=2, j=7, k=135\n",
      "a=data source that reported both msw and mcp. while this fitting method deviates from recommendations by kz07 regarding how the fit should be performed ( i. e., observations should be binned and then fit ), the focus here is on interagency and interannual differences rather than the absolute accuracy of any one formula. such an analysis shows how operational procedures might have changed over time. the wpr parameters are shown in fig. 3 as a time series along with the root - mean - square ( rms ) error of the empirical fit and v 920, the wind speed corresponding to mcp 5 920\n",
      "g=IBTrACS|International Best Track Archive for Climate Stewardship\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=les; these two tasks require different types of capital, production inputs, and labor skills, which inhibit the farms' ability to diversify across these product categories.\n",
      "Only 6.64% of the farms in the sample were located in an urban area. The concept of a CSA program is grounded in linking consumers to their food source and farmers. As a result, CSA programs sometimes require members to \"volunteer\" on the farm. Some volunteers might help with the planting, others with weeding during the growing season and others with the harvest. Approximately 15% of the CSA farms in our sample required members to work on the farm a mandatory number of hours. Proposition 1 predicts that the price difference between yield shares and weight shares will be determined by two potentially competing factors, the yield premium and the risk premium.\n",
      "Given the anecdotal evidence discussed above, we expect that the yield premium will be near zero (though this is not testable directly in our data set) and that the risk premium will be positive (weight shares cost more) due to negative covariance of prices and yields. We conduct a regression analysis to test the resulting prediction that weight shares will be more costly, on average, and to estimate the extra cost under the assumption that the yield premium is zero.\n",
      "Specifically, we model the price of a share as a function of the share type while controlling for other factors that affect share price, including the products produced by the farm and offered through the CSA program, the farming practices, the pick-up location, the state where the CSA farm is located, and other outlets through which the farm sells its products. We estimate the following equation using ordinary least squares (OLS). Our econometric model is a basic hedonic pricing approach. Firm-level data, in this case the prices charged by the CSA farms, can only be used to estimate hedonic models if price-cost markups are first eliminated and the competitiveness of the market is\n",
      "i=3, j=440, k=478\n",
      "a=[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "g=Census of Agriculture\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=ristics that are relatively changeable and observable to students and faculty. Global school characteristics were included to adequately assess the effect of attending a Catholic school by controlling for other important global school characteristics.\n",
      "Variables in this category were Catholic school status, enrollment size, average pre-test scores, average parental SES, percentage of minority students, and institutional location (urban, suburban, or rural) . Aggregate pre-test scores and mean SES were treated as global school characteristics because these variables must be controlled to assess the effects of Catholic schools. Internal school characteristics were included to understand the reasons for any effects of Catholic schools. Examining internal characteristics can also help us determine the kind of school policy or environment that can positively or negatively affect students' academic development. The internal school variables were monitoring of academic progress, strictness of school rules, extent of school's encouragement for parental support and involvement, teachers' morale, students' morale, and teacher-student ratios. See Appendix A for the list of all the variables and their coding schemes. We began the analysis by generating descriptive statistics such as means, standard deviations, and correlations. Table 1 presents the means and standard deviations of variables included in HLM analysis, as well as the correlation coefficients between the variables and Catholic schools. Except for Catholic school, mean pretests, student's perception of each subject's usefulness, and parental education, the listed variables had significant positive or negative effects on at least one of the outcomes when included with other predictors in the HLM models.\n",
      "To test the null hypothesis that there is no significant difference in development of academic achievement in reading, mathematics, history/social studies, and science between Catholic and non-Catholic private secondar\n",
      "i=4, j=54, k=94\n",
      "a=average pre - test scores, average parental ses, percentage of minority students, and institutional location ( urban, suburban, or rural ). aggregate pre - test scores and mean ses were treated as\n",
      "g=Education Longitudinal Study|National Education Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ADNI: www.adni-info.org). A further limitation of the present work is the significantly younger age of the control group in comparison to the patient cohort. However, this aspect might only have contributed to the discrimination between dementia patients and control subjects but not to the high classification accuracy of AD and FTLD patients as these were very similar in their age range. If age contributed to the classification accuracy there should be lower classification accuracy for young dementia patients and older control subjects as they did not differ in age. For comparison of both types of dementia patients and control subjects the classification accuracy did not differ for younger and older dementia patients although half of the patients were in the same age range as the control group. In AD group all patients were classified correctly. In FTLD group one younger and one older patient were misclassified. Independently of age, all control subjects were classified correctly for both comparisons. These results indicate that the slight mean age differences is not the decisive factor for the high discrimination accuracy using combined information from FDG-PET and MRI. Furthermore, if age still slightly contributed to the high discrimination of dementia patients and control subjects this contribution was also present in all other single modality and multimodal whole-brain and ROI-based SVM classifications applied in this study. Therefore, age cannot account for increased differentiation accuracies when combined ROI information from FDG-PET and MRI are used for differentiation of dementia patients and control subjects.\n",
      "Another point is that subjects in the control group in our study reported subjective cognitive complaints which might have limited the interpretation of the results of our study. However, only subjects were included whose cognitive complaints were not confirmed by comprehensive neuropsychological evaluation. The CDR is a semi-structural interview and\n",
      "i=5, j=7, k=221\n",
      "a=##ni - info. org ). a further limitation of the present work is the significantly younger age of the control group in comparison to the patient cohort. however, this aspect might only have contributed to the discrimination between dementia patients and control subjects but not to the high classification accuracy of ad and ftld patients as these were very similar in their age range. if age contributed to the classification accuracy there should be lower classification accuracy for young dementia patients and older control subjects as they did not differ in age. for comparison of both types of dementia patients and control subjects the classification accuracy did not differ for younger and older dementia patients although half of the patients were in the same age range as the control group. in ad group all patients were classified correctly. in ftld group one younger and one older patient were misclassified. independently of age, all control subjects were classified correctly for both comparisons. these results indicate that the slight mean age differences is not the decisive factor for the high discrimination accuracy using combined information from fdg - pet and mri. furthermore, if age still slightly\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=rk's tag with profiles of water temperature from IMOS floats in the same region where the shark was resident. The maximum depth of descent was assumed to be the greatest depth in the water temperature profile where the minimum temperatures reported by the tag and those of the water temperature profile were the same. For the track with multiple home range cores (Shark 5), movement patterns were categorised as within and outside the 25% utilisation distribution. We then used generalised linear models with a binomial distribution and a logit link function to assess the relationship between the probability of the shark being in a home range core and water temperature, bathymetry and region of the WA coast (north-latitude < 24S and south-latitude >24S). We were not able to fit all three explanatory variables in one model as there was no overlap of the temperature ranges between the two regions. Consequently, we fitted a model to examine the probability of being in a home range core in relation to bathymetry and region and two separate models (using the data from north and south coasts, respectively) to examine the probability of the shark being in a home range core in relation to sea surface temperature. To address the autocorrelation present in the data we used a matched-block bootstrap sampling for all models with replacement procedure [71, 72] that resampled blocks of data randomly and then recombined them in a random order, creating a bootstrapped dataset that minimized the effect of autocorrelation [71] [72] [73] . Model fitting was applied to 100 bootstrapped samples and model selection used the sample-corrected Akaike's information criterion (AIC c ), AIC c weight ( w AIC c ), and percent deviance explained (%DE) [74, 75] . Bathymetry data with a grid resolution of 2' from ETOPO1 database hosted by the NOAA was obtained by the R software package marmap [76] . Daily Sea Surface Temperature was obtained through the daily Optimum Interpolation Sea Surface Temperature\n",
      "i=6, j=195, k=278\n",
      "a=the probability of being in a home range core in relation to bathymetry and region and two separate models ( using the data from north and south coasts, respectively ) to examine the probability of the shark being in a home range core in relation to sea surface temperature. to address the autocorrelation present in the data we used a matched - block bootstrap sampling for all models with replacement procedure [ 71,\n",
      "g=Optimum Interpolation Sea Surface Temperature\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c= at the 1% level of significance thereby supporting the Alternate Hypothesis that the integration of economics in the curriculum has a statistically significant impact on economics outcome scores. Taking this analysis one step further we determined, again at the 1% level of significance, that in 1994, school systems that had integrated economics education in the curriculum scored on average 6 points higher than other school systems both at grade level 5 and 8; and 4 points higher at grade level 3. In 1995, it was deter-\n",
      "mined, again at the 1% level of significance that school systems that had integrated economics in the curriculum scored on average 8 points higher than other school systems at grade level 8; and 6 points higher at grade level 3 and 5. In 1997, it was determined, again at the 1% level of significance that school systems that had integrated economics in the curriculum scored on average 9 points higher than other school systems at grade level 3, 5, and 8. Finally, in 2000, it was determined that school systems that had integrated economics in the curriculum scored 4, 8, and 7 points higher than other school systems at grade level 3, 5, and 8, respectively. With mean economics outcome scores on the MSPAP often ranging in the twenties and thirties, a 4-9 point differential is significant. School systems where integration of economics in the curriculum is at one or less grade levels. Notes: * Significant at the 1% level Yes1: School systems with systematic integration of economics in the curriculum at 2 or more grade levels. No2 :\n",
      "School systems where integration of economics in the curriculum is at one or less grade levels. School systems where integration of economics in the curriculum is at one or less grade levels. Although the hypothesis testing undertaken in the previous section established a statistically significant correlation between economics outcome scores and the level of economics instruction it does not explain this relationship.\n",
      "In the case\n",
      "i=7, j=222, k=61\n",
      "a=IMPOSSIBLE\n",
      "g=National Assessment of Education Progress\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=VEL correction are more enriched for brain regions known to undergo structural changes in AD. Finally, we show that the average hippocampal intensity after RAVEL correction performs better than intensity-normalized-only images in discriminating between AD patients and healthy controls, and between MCI patients and healthy controls. This shows that RAVEL-corrected T1-w intensities are more biologically meaningful than intensity-normalized-only images for group comparisons, and therefore potentially promising for the development of biomarkers.\n",
      "Although we apply RAVEL in the context of T1-w MRI of the brain, our method is generalizable to many imaging modalities. In addition, the flexibility in the choice of the control voxels makes RAVEL applicable to any disease or pathology.  Our dataset consists of a subset of 917 subjects downloaded from the ADNI database (adni.loni.usc. edu). For each subject, we selected a study visit at random. We obtained 506, 184 and 227 subjects from the ADNI, ADNI-2 and ADNI-GO phases, respectively. We present summary statistics of the study population in Table 1 . The selected scans were acquired at 83 different imaging sites, with a median number of 10 patients per site. The scans were also well-balanced for disease status across sites.  We considered T1-w imaging acquired on 1.5 and 3 T scanners according to the ADNI standardized protocol . All analysis was performed in R [R Core Team, 2014] , using the packages oro.nifti [Whitcher et al., 2011] , fslr , ANTsR [Avants et al., 2015] and WhiteStripe [Shinohara and Muschelli, 2015] .\n",
      "We applied the N4 inhomogeneity correction algorithm [Tustison et al., 2010 ] to each image. We nonlinearly registered all T1-w images to a high-resolution T1-w image atlas [Oishi et al., 2010] , using the symmetric diffeomorphic image registration algorithm [Avants et al., 2008] implemented in the ANTs suite. We use non-linear registration in order to define a brain control region aligned across subjects and to fi\n",
      "i=8, j=283, k=31\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=ential neighborhoods and some commercial development. Though some areas of the creek are open to shell fishing (at the lower reaches of the creek), Pages Creek has experienced a decline in the overall water quality. This creek has also seen an increase in bacteria levels during and immediately after rain events, indicating the impacts of storm water runoff from impervious surfaces [23] . As a result, Pages Creek is routinely monitored and regulated through a joint effort between federal and state agencies. [5] method may provide improvement to bathymetric mapping.\n",
      "Pages Creek is located in southeastern North Carolina, United States, and drains approximately 4100 acres into the Atlantic Intracoastal Waterway (ICW). Approximately 4500 people live within the boundary of this watershed (Figure 1 ). The Pages Creek watershed consists mostly of residential neighborhoods and some commercial development. Though some areas of the creek are open to shell fishing (at the lower reaches of the creek), Pages Creek has experienced a decline in the overall water quality. This creek has also seen an increase in bacteria levels during and immediately after rain events, indicating the impacts of storm water runoff from impervious surfaces [23] . As a result, Pages Creek is routinely monitored and regulated through a joint effort between federal and state agencies.  The application of remote sensing analyses to map marsh habitats and shallow water bathymetry has not been done using this combination of fieldwork, WV-2 imagery, and LiDAR data. \n",
      "The application of remote sensing analyses to map marsh habitats and shallow water bathymetry has not been done using this combination of fieldwork, WV-2 imagery, and LiDAR data. Similar studies have successfully conducted mapping of marsh habitats using various combinations of imagery and classification methods; however, it has been difficult to distinguish between salt marsh species because of spectral similarities in small geographic areas [2, \n",
      "i=9, j=80, k=126\n",
      "a=creek is routinely monitored and regulated through a joint effort between federal and state agencies. [ 5 ] method may provide improvement to bathymetric mapping. pages creek is located in southeastern north carolina, united states, and drains approximately 4100\n",
      "g=NOAA Tide Gauge\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=en the front is further east could mean reduced availability of krill for the predators. Our analysis of historical hydrographic data has shown the mean position of the SACCF to be much further east than in other work; this makes fluctuations in the western extent of the front much more important in terms of delivering krill into regions accessible by the predators. Eddies shed from the front may also transport krill into the region, thus the timing and frequency of these events and the fate of the eddies may have an impact on the South Georgia ecosystem. The resolution of the close proximity of the SACCF to the southern shelf of South Georgia shows that the SACCF may play an important role in transport to the southern South Georgia region. Once material reaches the South Georgia shelf, on-shelf processes such as tides may act to retain or circulate it on-shelf. These processes could provide an alternative transport route for passive drifters to reach the western end of South Georgia. Finally, a note on the dynamics of the system. The short period of model output, with only two occurrences of the large westward shift in the position of the retroflection, means that we cannot definitively determine the mechanisms for the westward extensions of the SACCF and the related eddy-shedding. However, the wind forcing in the run of OCCAM considered here is the only forcing that is not cyclical (recall that surface temperature and salinity are relaxed to monthly climatological Levitus 1994 data, Section 3.1). The infrequency of the large extensions of the SACCF retroflection suggests that there might be a signal in the wind stress that predisposes the front to move zonally. Time series of the mean zonal and meridional wind stress averaged over the Scotia Sea (50 -65jS, 70 -30jW) and over the study region, however, show no consistent feature that corresponds to the westward extensions of the SACCF. This suggests that wind stress does not play a direct role in triggering the mov\n",
      "i=10, j=3, k=25\n",
      "a=front is further east could mean reduced availability of krill for the predators. our analysis of historical hydrographic data\n",
      "g=World Ocean Database\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ars. For up-to-date information, see www.adni-info.org. Full details of subject recruitment, PET scanning protocols, and data preprocessing were published elsewhere [23, 42, 44] (http://www.loni. ucla.edu/ADNI/) and only a brief account is given here. Study subjects gave written informed consent at the time of enrollment for imaging and genetic sample collection and completed questionnaires approved by each participating sites Institutional Review Board (IRB). For more information, please refer to the ADNI website (http://www.adni-info.org). The authors state that they have obtained approval from the ADNI Data Sharing and Publications Committee for use of the data and confirm that the data was analyzed anonymously. The data used in our study come from a subset of AD, MCI, and cognitively normal ADNI participants who had completed at least two visits at the time of this study and fulfilled other criteria explained below. For ADNI full inclusion/exclusion criteria see http://www.adni-info.org.\n",
      "Subjects between the ages of 55 and 90 were enrolled in the ADNI study. Eligibility criteria were as follows (see Petersen et al., 2010 , for a description of participant recruitment and classification protocol). Normal elderly controls had a Mini Mental State Examination (MMSE) score of 24 or higher [45] , a Clinical Dementia Rating (CDR) of 0 [46] , and no diagnosis of neurological disease or psychiatric disorder. MCI patients had a MMSE score of 24 or higher, a subjective memory complaint, objective memory loss measured by education adjusted scores on the Wechsler Memory Scale Logical Memory II, a CDR score of 0.5, absence of significant levels of impairment in other cognitive domains, preserved activities of daily living (ADLs), and an absence of dementia [47] . Participants with mild AD were enrolled if they had a MMSE score between 20 and 26 (inclusive), a CDR score of 0.5 or 1.0, and met NINCDS-ADRDA criteria for probable AD [48] .\n",
      "At the initiation of our study, 489 ADNI\n",
      "i=11, j=286, k=38\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=that readily available and easily accessible healthful foods within the home are likely to enhance healthful dietary intake among youth and families.\n",
      "Availability of soft drinks in the home has also been strongly associated with softdrink consumption among children (42) . A recent home-based environmental pilot study was conducted through weekly home deliveries of noncaloric beverages to displace sugarsweetened beverages (SSBs) to reduce SSB consumption among adolescents, who were frequent consumers of SSB (23) . The results of this relatively simple environmental intervention showed that SSB intake decreased in the intervention group, and investigators saw a significant body mass index (BMI) change among adolescents in the highest BMI tertile group.\n",
      "Social-environmental influences within the home such as modeling of healthful dietary intake by parents and siblings, authoritative feeding style (i.e., high in limit setting but also high in nurturance), and more (18, 27, 44) and may be the strongest predictor of fruit and vegetable consumption among young children (18) . A recent systematic review by van der Horst and colleagues (100) report an association between parent and child intake of fat, fruits, vegetables, and soft drinks.\n",
      "Another factor that may influence children's dietary intake is parental feeding style and parenting practices. An authoritative feeding style has been positively associated with preschool children's intake of dairy and vegetables (73) , and mother's authoritative parenting style is associated with adolescent intake of fruits and vegetables (63) . Birch (5) found that parental practices such as restricting foods, pressuring children to eat, or using foods as rewards may inadvertently promote behaviors counter to their intentions. For example, parental pressure could result in decreased preference for certain foods, whereas food restriction could increase preferences for certain foods.\n",
      "Frequency of family meals may also have a positive impact\n",
      "i=12, j=167, k=125\n",
      "a=IMPOSSIBLE\n",
      "g=Early Childhood Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=o a first-generation antipsychotic comparator and assessed at multiple time points (e.g., [28, 29] ). Thus, there is substantial evidence that the influences of these practice effects had unfortunate consequences, as a confusing state evolved with claims and counterclaims made for the \"benefits\" of one antipsychotic or another. It is our hope that by drawing attention to this issue, the field will begin to develop novel strategies that may overcome changes in cognitive performance that are solely due to practice effects. An organized attempt to reduce practice effects will not only eliminate a large source of noise in AD trials but also reduce the likelihood of misinterpreting outcomes. As such, we believe that this issue should be dealt with proactively in the design of clinical trials.\n",
      "Critically, given our review of the literature, it is likely that individuals who are at high risk for AD (symptomatic or asymptomatic) and who are enrolled in trials and have intact cognition or subtly impaired cognition will demonstrate robust practice effects on many of the tests used. If not controlled, these practice effects may mask subtle decline in a placebo group, reducing the ability to detect improvements, if any, in the active treatment group. In this latter group, conservatively, an ES of 0.25 units would likely be larger than decline over a 6-to12-month period. Alternatively, effects might be misinterpreted as active drug effects in trials in which cohort differences in learning were present and positive drug effects would have to be substantial to be detected (see below). Several solutions to the problem of practice effects can be considered (Table 2) . On the face of it, the use of alternate forms would seem to be a straightforward approach [30] . However, some tests with problem-solving components (e.g., the Wisconsin Card Sorting Test and similar procedures) and that require discovery of an overarching and discrete set of rules are Test (AVLT) in the non-cognitivel\n",
      "i=13, j=4, k=3\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=onstrated that more complicated networks including several convolutional layers do not necessarily produce higher accuracy rates and that trade-off between network complexity and performance of classification must be achieved through a valid hypothesis, for example, the input dimensions. Three layers of convolution with three pooling layers followed by two fully-connected layers were utilized in MCADNNet topology (https://github.com/samansarraf/MCADNNet). Finally, a softmax layer to classify three classes was added to the end of the network. Three convolutional layers were designed to extract deep but hierarchical features from data. Figure 1 images the MCADNNet architecture. In this topology design, functional and structural MRI samples were upsampled to 5656 pixels, the closest dimension to the original image size after data conversion. The upsampled images were fed into the first convolution layer that contains 10 filters of 55. In the second layer, the first max pooling layer downsampled the data by a factor of two. Next, in the third layer that is the second convolution layer, the features were passed through 20 filters of 55. As we will see later, the first Conv. layer extracted high-level features. After, the second max pooling layer downsampled the outcome of the second conv. layer which were mid-level features by a factor of two. The final convolution layer (the 5th layer) generated the low-level features to feed the last pooling layer. Two consecutive fully-connected layers were learned from the hierarchical features and transferred the output to the softmax layer for multiclass classification. Increasing the number of convolutional layers as well as the number of filters generated a higher number of network parameters. To avoid any potential overfitting or extraction of various features from the data, the pooling layers were utilized, which also accelerated the training process.As described earlier, DeepAD and MCADNNet were trained from scratch using ADNI\n",
      "i=14, j=0, k=78\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c= the median time under review is 85 days. However, there is no significant correlation between time under review and the paper's IDR score, and the IDR score does not predict time under review. Confidence in this finding is buttressed by an analysis of data on unpublished working papers that we collected from www.ArXiv.org. We searched for working papers by the 854 authors in our sample, and were able to locate 220 papers written by 63 authors (see Table A4). Stripping all references from these working papers, identifying each referenced journal, and matching the journals with WoS SCs allowed us to calculate IDR scores for each working paper. Comparisons of the 220 working papers with the 3983 published papers by the same authors reveal that working papers are, indeed, more interdisciplinary, and this may make them more likely to be file-drawered or rejected, thereby contributing to the productivity penalty. But because working papers are more recent (average posting date in ArXiv.org is 2009, compared to an average publication date of 1997 for the published papers), and may be more interdisciplinary simply given the upward trend in IDR (documented by Porter and Rafols 2012, and confirmed with our data in Figure 3), it is difficult to interpret this difference. As a more stringent test, we took a closer look at our sample of working papers. For each working paper, we identified which papers were subsequently published (using author updates on www.ArXiv.org and online searches for each paper). We compared working papers that eventually got published with those that did not. Here, the eventually published papers (n=122) are actually more interdisciplinary than the still unpublished papers (n=115), a difference significant at p=.054 (two-tailed), suggesting that IDR papers are not hindered in the review process. ~ Table A4 and Figure 3 about here ~ Taken together, these supplementary data and analyses support our theorizing about cognitive and collaborative challenges \n",
      "i=15, j=405, k=358\n",
      "a=IMPOSSIBLE\n",
      "g=Survey of Doctorate Recipients|Survey of Earned Doctorates\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=robabilities even as low as 1-3% can lead to substantial overestimation of occupancy (Royle and Link 2006, Miller et al. 2011 ) and bias in estimators of colonization and extinction (McClintock et al. 2010a ). For example, under a broad set of simulated conditions, Miller et al. (2011) found that when false positive detections occurred only 1% of the time, the standard occupancy estimator (MacKenzie et al. 2006) was positively biased by an average of 0.06 (and up to 0.29) . This bias becomes even more severe with higher false positive detection probabilities. Using field data from pickerel frog (Lithobates palustris) call surveys, Miller et al. (2011) found that occupancy estimates were 2.15 times greater when false positive detections were not accounted for. Biases that result from false positive errors are greatest when species are rare (Miller et al. 2011) . The costs associated with inaccurate inference owing to these errors can be substantial (Dalton 2009 , Roberts et al. 2010 Many important insights about auditory surveys have come from recent studies using an automatic playback system that enables field conditions to be simulated, originally developed by Simons et al. (2007) . These studies have extended our understanding of the detection process and the validity of statistical estimators that attempt to account for imperfect detection (Simons et al. 2007 , Alldredge et al. 2008 , McClintock et al. 2010a among others) . A key finding of these studies, as well as others (e.g., Bart 1985 , Genet and Sargent 2003 , Lotz and Allen 2007 , Campbell and Francis 2011 , Farmer et al. 2012 , is that false positive errors are more common than generally acknowledged. McClintock et al. (2010a, b) was the first to examine false positive detections in detail. They found that false positive error probabilities varied greatly among observers and among species played in the study, false positive errors rates were affected by distance from a call and ambient noise, and that err\n",
      "i=16, j=427, k=126\n",
      "a=IMPOSSIBLE\n",
      "g=North American Breeding Bird Survey\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c= validation techniques. Also, when dealing specifically with hippocampal segmentation the differences in segmentation protocols represent a particularly limiting constraint (Bellotti and Pascazio 2012 , Nestor et al 2013 . Although in recent years a considerable effort has been invested in the creation of a unified segmentation protocol (Frisoni and Jack 2011, Frisoni et al 2015) , consensus has not been reached. Therefore, a segmentation algorithm with the ability to adapt to different protocols is very desirable.\n",
      "Based on the previous considerations, in this paper we present a novel and fully automated hippocampal segmentation algorithm, named HUMAN (Hippocampal Unified Multi-AtlasNetworks), which combines in a unified framework the accuracy of multi-atlas methods with the robustness of artificial neural networks classification. The performance of this methodology was assessed with two independent test sets, segmented with two independent protocols. The first set was provided by the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the second one by the Open Access Series of Imaging Studies (OASIS). Different manifold strategies for atlas selection were explored in order to identify an optimal setup for learning. The performance of HUMAN when applied to sub-regions ( patches) of the hippocampus was then investigated. Finally, HUMAN was compared to the publicly available segmentation tool FreeSurfer (Fischl 2012) and to a basic multi-atlas pipeline, consisting of registration and label fusion. A data set of 100 T1 MRI scans from the ADNI database (1.5 T and 3.0 T), including normal control (NC), mild cognitive impairment (MCI) and Alzheimer's disease (AD) subjects, was used in preparation of this article. The relative hippocampal labels were provided by the EADC-ADNI harmonized segmentation protocol 7 (Boccardi et al 2015a (Boccardi et al , 2015b were matched in terms of demographic and clinical composition. The volume distributions in the training set and test set were also matched, thus excluding any volume\n",
      "i=17, j=489, k=5\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ADNI). In our study, 40 patients with mild cognitive impairment (MCI) who later developed to probable Alzheimer's disease (AD) and, for comparison, 15 normal controls (NC) were selected. In MCI group, the average MMSE decline is 9.2 (within 23 years), which indicates a substantial neuropsychological disorder has been developed. Therefore, the corresponding decrease of the cortical thickness is expected. In NC group, the average MMSE change is 1.3. Considering the possible MMSE assessment errors, subjects in this group can be regarded as neuropsychological healthy, and thus the cortical thickness is expected to keep stable (or slight decrease with normal aging). Since the 4D processing is introduced at both the segmentation step and the thickness measuring step, totally four different thickness measuring pipelines are compared in order to trace the source of the possible observed improvements. The four different combinations are: 3D segmentation and 3D thickness measurement (3S3T), 3D segmentation and 4D thickness measurement (3S4T), 4D segmentation and 3D thickness measurement (4S3T) and 4D segmentation and 4D thickness measurement (4S4T). After the 4 different thickness values are measured, in order to conduct voxel-wise group analysis, each subject's thickness maps are mapped onto the template space. Results on MCI group. Before the voxel-wise correlation analysis can be conducted in the template space, the mapped thickness is first smoothed using full-width-at-half-maximum (FWHM) Gaussian filter ( = 8mm) in order to suppress possible registration errors and inter-subject structure variations. After that, Pearson's correlation between thickness and MMSE score is calculated voxel-wise within each subject. The average correlation within the MCI group are then computed by transforming the correlation coefficients to Fisher's z -value and transforming back. The resultant average correlations from the four methods are summarized in Fig. 3 . As we can see, when more 4D \n",
      "i=18, j=177, k=106\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI|Alzheimers Disease Neuroimaging Initiative\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=raining over for example the teaching function. A large proportion of research funding in US universities and certainly the most prestigious are from national institutes such as NSF and NIH. The level of US research expenditures is thus related direct US government policy and indirectly through R and D tax credits (Bloom, Griffith et al. 2002). Consequently, individuals mostly funded by research grants, fellowships and research assistantships are not only likely to develop better research skills, there are more likely to become embedded in higher quality research network. Thus: H4: Individuals whose primary source of funding for their degree focus on research training and activities are more likely to be embedded in higher quality research networks and less likely to return home, all else equal. Individuals often forego economic benefits for cultural and familial reasons. Thus we expect the extent to which individuals are socially and culturally connect to their home countries it is likely to increase their likelihood of returning home. While simple to state this is very complex. It can include such diverse issues as social constructed gender roles, family demographics (e.g. marital status), and connection to social networks through prior institutional affiliations (e.g. number of prior higher education degrees from their home country). Empirical efforts to deal with these factor are typically done through the use of numerous control variables like gender, marital status, number of higher education degrees from their home country, and country specific effects. It is important to understand in the context of the current study, the vast majority of non-US Next we are interested in how science policy might affect the individual decision to return. Here we suggest several possible mechanisms. The first is direct subsidy programs offered to an individual to study abroad. If a major source of funding for non-US citizens is from foreign sources, which are most likely to be\n",
      "i=19, j=2, k=375\n",
      "a=over for example the teaching function. a large proportion of research funding in us universities and certainly the most prestigious are from national institutes such as nsf and nih. the level of us research expenditures is thus related direct us government policy and indirectly through r and d tax credits ( bloom, griffith et al. 2002 ). consequently, individuals mostly funded by research grants, fellowships and research assistantships are not only likely to develop better research skills, there are more likely to become embedded in higher quality research network. thus : h4 : individuals whose primary source of funding for their degree focus on research training and activities are more likely to be embedded in higher quality research networks and less likely to return home, all else equal. individuals often forego economic benefits for cultural and familial reasons. thus we expect the extent to which individuals are socially and culturally connect to their home countries it is likely to increase their likelihood of returning home. while simple to state this is very complex. it can include such diverse issues as social constructed gender roles, family demographics ( e. g. marital status ), and connection to social networks through prior institutional affiliations ( e. g. number of prior higher education degrees from their home country ). empirical efforts to deal with these factor are typically done through the use of numerous control variables like gender, marital status, number of higher education degrees from their home country, and country specific effects. it is important to understand in the context of the current study, the vast majority of non - us next we are interested in how science policy might affect the individual decision to return. here we suggest several possible mechanisms. the first is direct subsidy programs offered to an individual to study abroad. if a major source of funding for non - us citizens is from foreign sources, which are most likely to be [SEP] what dataset [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "g=Survey of Doctorate Recipients|Survey of Earned Doctorates\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=rebellum.\n",
      "11 of 15 measured metabolites were shown to be associated with disease.\n",
      "Decreases in dopamine were seen in the ASYMAD group in the MFG when compared to control and AD patients (FC=0.78, p=4.110 -3 ). In AD patients changes were mainly seen in the ITG's inhibitory GABAergic system.\n",
      "Alzheimer's pathology but intact cognition, while and imbalance of several neurotransmitters is evident in the brain of AD patients. Dementia is a devastating illness for both patients and their families, with Alzheimer's disease (AD) estimated to account for up to 80% of total dementia cases. The 'World Alzheimer's report 2015' estimates that there are approximately 46 million AD patients worldwide, with this number expected to rise to over 130 million by the middle of the century (1) . As well as a significant human cost, AD also represents a major financial burden with worldwide costs related to AD expected to reach $1 trillion dollars in 2018 (1) .\n",
      "Cholinesterase inhibitors make up three of the four approved AD treatments (Donepezil, Rivastigmine and Galantamine) making inhibition of acetylcholinesterase the leading therapeutic strategy for the treatment of AD symptoms (2, 3) . There is a significant body of literature that has suggested that the cognitive deficits associated with Alzheimer's disease are the result of lower levels of acetylcholine in the brain resulting from dysfunction of cholinergic neurons (4) (5) (6) . The role of non-cholinergic neurotransmitter systems in AD pathogenesis has received less attention. While levels of non-cholinergic neurotransmitters in the brain have been associated with Alzheimer's pathology (7) (8) (9) (10) (11) , their role in mediating the onset of symptoms is less well understood. In this study, we analysed data from non-targeted metabolomics to compare differences in neurotransmitters and neurotransmitter-associated metabolite levels in brain tissue samples from the autopsy cohort of the Baltimore Longitudinal Study of Aging (BLSA)\n",
      "i=20, j=3, k=6\n",
      "a=. 11 of\n",
      "g=Baltimore Longitudinal Study of Aging|Baltimore Longitudinal Study of Aging (BLSA)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=mmunity Household Panel (ECHP), the International Social Survey Program (ISSP) and the International Adult Literacy Survey (IALS), the authors produced a sample that covered 24 countries. They found that early tracking reinforced the effect of family background on years of education, probability of dropping out, likelihood of completing college, and earnings. Interestingly, while literacy is positively correlated with parental education, tracking actually decreased this correlation. The authors concluded that tracking generally increases the effect of parental background on educational outcomes, and that this effect persists following entry into the labor market. Schutz, Ursprung, and Womann (2008) hypothesized that the family background effect on student achievement would be greatest in school systems where the age of initial sorting into ability tracks is lowest. To measure student achievement, they used mean math and science scores from both the 1995 and 1999 administrations of the Trends in International Mathematics and Science Study (TIMSS). Combining data from the two rounds of testing, the authors compiled a data set containing data on more than 300,000 eighth-grade students from 54 countries. As a proxy for family background, Schutz, Ursprung and Womann used the number of books in the student's home, as reported by the student in the student background portion of TIMSS. The data showed that, in line with their hypothesis, the size of the family background effect on student achievement was inversely related to the age at which students were first separated into tracks based on ability. Brunello and Checchi (2007) and Schutz, Ursprung and Womann (2008) came to similar conclusions in their studies: educational tracking increases the effect of family background on student achievement and therefore increases educational inequality. Both sets of authors recommend less differentiated education systems to promote equality of educational opportunity. However, these studies and others like them (see Horn, 2009; Bauer an\n",
      "i=21, j=3, k=389\n",
      "a=##ty household panel ( echp ), the international social survey program ( issp ) and the international adult literacy survey ( ials ), the authors produced a sample that covered 24 countries. they found that early tracking reinforced the effect of family background on years of education, probability of dropping out, likelihood of completing college, and earnings. interestingly, while literacy is positively correlated with parental education, tracking actually decreased this correlation. the authors concluded that tracking generally increases the effect of parental background on educational outcomes, and that this effect persists following entry into the labor market. schutz, ursprung, and womann ( 2008 ) hypothesized that the family background effect on student achievement would be greatest in school systems where the age of initial sorting into ability tracks is lowest. to measure student achievement, they used mean math and science scores from both the 1995 and 1999 administrations of the trends in international mathematics and science study ( timss ). combining data from the two rounds of testing, the authors compiled a data set containing data on more than 300, 000 eighth - grade students from 54 countries. as a proxy for family background, schutz, ursprung and womann used the number of books in the student's home, as reported by the student in the student background portion of timss. the data showed that, in line with their hypothesis, the size of the family background effect on student achievement was inversely related to the age at which students were first separated into tracks based on ability. brunello and checchi ( 2007 ) and schutz, ursprung and womann ( 2008 ) came to similar conclusions in their studies : educational tracking increases the effect of family background on student achievement and therefore increases educational inequality. both sets of authors recommend less differentiated education systems to promote equality of educational opportunity. however, these studies and others like them ( see horn, 2009\n",
      "g=Education Longitudinal Study|Trends in International Mathematics and Science Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ADNI (Mueller et al., 2005;  Alzheimer's Disease Neuroimaging Initiative, www.adni-info.org) and AddNeuroMed (Westman et al., 2011, Innovative Medicines (InnoMed) in Europe) are comprised of AD cases, MCI, and aged-matched controls ( Table 1) . All AD cases met criteria for either probable or definite AD with inclusion criteria as previously described (Simmons et al., 2011 and www.adni-info.org) . MCI was assessed as having an abnormal memory complaint but with general cognition and functional performance sufficiently preserved such that a diagnosis of AD cannot be made. Elderly controls were screened for dementia. The population cohorts were the Older Australian Twins Study (OATS; Sachdev et al., 2011) , the Sydney Memory and Ageing Study (MAS; Brodaty et al., 2012) , and the Queensland Twin Imaging (QTIM) cohort, of which the latter consists of young adults . For Sydney MAS and OATS, diagnosis of MCI and AD were made with the most recent consensus criteria (Winblad et al., 2004) . For all those participants whose neuropsychological or functional profiles indicated the possibility of dementia, a diagnosis was made at a consensus meeting (for a detailed description of Sydney MAS and OATS methodologies see Sachdev et al., 2009 Sachdev et al., , 2012 . Both Sydney MAS and OATS are longitudinal studies; here, we used the MRI data and diagnosis of MCI or AD at baseline (i.e., on admission).\n",
      "All cohorts are independent of the IGAP GWASMA except for ADNI, which contributed 1.6% of the AD cases and 0.5% of the controls . In this study, we formed an all-older group, aged 53e91 years, from 4 cohorts (ADNI, AddNeuroMed, OATS, and Sydney MAS), and stratified into AD, MCI, and healthy older groups. The QTIM cohort (aged 16e30 years) formed a separate young adult group (Table 1) . As OATS and QTIM cohorts contain twins, we omitted related individuals at random. Subcortical volumes\n",
      "i=22, j=2, k=13\n",
      "a=##ni ( mueller et al., 2005 ; alzheimer '\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ADNI1, PPMI1, and PPMI2 (p-values are ADNI1 versus PPMI1 : 0.68, PPMI1 versus PPMI2 : 0.25, ADNI1 versus PPMI2 : 056). For the extended comparison between ADNI1 and PPMI2 as well, no statistical difference was observed after adjusting for age and gender (p = 0.92). (2c) Fig. 2 . Agreement between different software packages for subcortical segmentation. While large (>3000 mm 3 ) structures with distinct gray matter-white matter contrast such as the thalamus are well-delineated and consistent among software, segmentation of smaller structures such as the amygdala shows considerable variability and bias depending on the software used. The plots for Caudate, Putamen, Hippocampus, Pallidum were similar to Thalamus. However, the putamen showed a proportional error between FreeSurfer and Multi-atlas measurements. Accumbens, which is a small structure and has no discernible gray-white matter contrast, showed surprisingly good agreement between software packages. We acknowledge that for an ideal comparison, the same subjects should be scanned using the different scanner manufacturers and imaging parameters. We have used three groups of healthy control subjects of similar ages and sex distribution with the assumption that each group will have similar morphometric measurements. Han et al. [8] , showed in their study that cortical thickness measurements varied by 0.12 mm across scanner manufacturers (Siemens versus GE) using the same subjects [20] . Difference within the same scanner manufacturers was about 0.03. Within (Siemens) scanner difference in cortical thickness was similar in our study (0.02  0.04 mm). Our results show a difference in cortical thickness between scanners (Siemens versus Philips) of 0.05  0.07 mm.\n",
      "In the subcortical structures, studies by the same group concluded that different scanner manufacturers did not show significant difference in the amygdala or the thalamus but there was a bias in the hippocampal measurements. In our study, there was a medium-s\n",
      "i=23, j=57, k=219\n",
      "a=comparison between adni1 and ppmi2 as well, no statistical difference was observed after adjusting for age and gender ( p = 0. 92 ). ( 2c ) fig. 2. agreement between different software packages for subcortical segmentation. while large ( > 3000 mm 3 ) structures with distinct gray matter - white matter contrast such as the thalamus are well - delineated and consistent among software, segmentation of smaller structures such as the amygdala shows considerable variability and bias depending on the software used. the plots for caudate, putamen, hippocampus, pallidum were similar to thalamus. however, the putamen showed a proportional error between freesurfer and multi - atlas measurements. accumbens, which is a small structure and\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=tend to produce more women chief executives than non-democracies (not free countries), as well. If authoritarian (not free) regimes are more likely to manage pandemics according to different incentives, and to promote women in leadership positions for reasons different from free countries, we must include them in any empirical analysis of gender, leadership, and pandemic outcomes.\n",
      "The importance of preparation. Second, critical to this debate is the fact that disaster outcomes are not merely dependent on leadership now, but also on preparation that took place previously [36, 37] . Strong disaster preparedness systems would have protocols in place for confronting threats such as a pandemic that today's leaders need only activate, adapt, and implement, meaning that women leaders today will be better placed to manage pandemics if they can rely on preparedness and policies made by leaders in the past. Consider another perspective from the disaster preparedness framework. To gauge general preparedness we calculate standardized average disaster preparedness scores according to the Hyogo Framework for Action [38] , and graph this by current leader gender as shown in Fig 2. This snapshot of pandemic management and leadership gender does suggest that women-led countries have higher average preparedness ratings than men-led countries.\n",
      "A surface-level interpretation of the graph might suggest that preparedness levels depend on leader gender, and that women lead countries that are more prepared because of the woman's leadership. But the timing of disaster preparation as, by definition, occurring before a critical event like a pandemic suggests that the relationship between disaster preparedness and leader gender is more nuanced-that in fact, some countries are more likely both to be prepared for disaster and also to elect women to leadership in government. We posit that the common factor among these countries is cultural.\n",
      "Feminine social cultural norms. Third, cultural norms, b\n",
      "i=24, j=476, k=3\n",
      "a=IMPOSSIBLE\n",
      "g=Our World in Data\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=and genetics have made it possible, and financially feasible, to scan populations with multi-modality brain imaging and collect genome-wide genotype data [1, 2] . The Alzheimer's Disease Neuroimaging Initiative (ADNI) recently acquired genome-wide genotype data and structural MRI scans from 818 subjects. This wealth of data presents powerful and unprecedented spatial and genetic resolution to detect specific variants that influence the brain. However, it requires new ways to deal with the computational load and account statistically for multiple comparisons across the genome and across the images. For the first time, we conducted a voxelwise genome-wide association study (vGWAS) to discover genes influencing brain structure across the entire brain. Each genetic variant identified is a potential candidate with the ability to affect brain structure. Neuroimaging and genetic data were acquired from 818 subjects as part of the Alzheimer's Disease Neuroimaging Initiative (www.loni.ucla.edu/ADNI). Only unrelated Caucasian subjects (non-Hispanic; N=740) identified by self-report and confirmed by MDS analysis were included to reduce population stratification effects. Volumetric brain differences were assessed in 173 AD patients (78 female/95 male; mean age  standard deviation = 75.54  7.66), 361 MCI subjects (130 female/231 male; 75.16  7.29), and 206 healthy elderly subjects (112 male/94 female; 76.13  4.94). 3D T1-weighted baseline brain MRI scans were analyzed using tensor-based morphometry (TBM) to measure volume differences relative to a standard template [3, 4] .\n",
      "Genome-wide genotype information was collected at 620,901 markers. We conducted a genomewide association analysis using volume differences relative to a mean brain image template at each voxel as a phenotype, assuming an additive genetic model and controlling statistically for age and sex. We selected only the most associated SNP at each voxel, saving its P-value and identifier. The effective number of tests was \n",
      "i=25, j=198, k=73\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=dunes and back-barrier marsh was the most efficient in terms of effectiveness and cost to manage and mitigate long-term barrier-island erosion; (2) the use of segmented breakwaters and rubble-mound seawalls was less effective and more costly than the use of dredged material and vegetation; and (3) improvements were needed in the final design templates to ensure that the slopes and elevations were correct for back-barrier marsh creation. Many of the findings in Penland and others (2003) have since been incorporated into subsequent CWPPRA and other restoration projects. Since the early 1980s, the USGS, the LGS, Louisiana State University (LSU), University of New Orleans (UNO), and other State and Federal agencies working in coastal Louisiana have compiled large volumes of data. Some examples include nearshore to offshore geophysical surveys (USGS, LGS, LSU, and UNO), photo mosaics, repeat lidar surveys (USGS and USACE), and the USACE-funded Beneficial Use of Dredged-Material Monitoring Program (2010). Some of these data were incorporated into existing national Geographic Information Systems (GIS) databases such as usSEABED (USGS; http://walrus.wr.usgs.gov/usseabed/) and regional publications such the Environmental Atlas of the Lake Pontchartrain Basin (Penland and others, 2002a). In an effort to facilitate the integration and dissemination of these project-specific data for scientific research, the USGS developed the Louisiana Sedimentary and Environmental Database (LASED; http://coastal.er.usgs.gov/lased/), and the Louisiana Department of Natural Resources developed the Strategic Online Natural-Resources Information System (SONRIS). Historical relative sea-level rise (RSLR) in coastal Louisiana is among the most rapid in the continental United States (Zervas, 2001) and many studies have documented historical land-loss trends and habitat changes in coastal Louisiana (for example, Cahoon and Groat, 1990;Reed, 1995;Barras and others, 2008;and Linscombe and Hartley, 2011\n",
      "i=26, j=202, k=25\n",
      "a=IMPOSSIBLE\n",
      "g=SLOSH model\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=iance of the truncated normal are functions of the covariates. We estimated only the mean as a function of the covariates. Specifying g as a linear function, g=Z* such that  i = Z i *, and defining =  i / i , and = ()/() where  is the normal probability function and  is the normal cumulative function allows computing the expected marginal efficiency impact of a variable x k on farm i as:\n",
      "where the term (1-*- 2 ) will vary by farm, but  k will be constant across farms.\n",
      "The frontier and efficiency components of equation (1) were estimated jointly using Maximum Likelihood Estimation. The data had been collected using a stratified random sample with an enhanced sample of larger farms since few large farms would have been surveyed with a random sample. Since a stratified random sample was used, a weighted maximum likelihood model was employed with the weights applied outside the likelihood value of each observation. Data are from the Dairy Production Practices and Costs and Returns Report (Agricultural Resource Management Survey Phase II, commonly referred to as ARMS).\n",
      "These data were collected by a survey jointly administered by the National Agricultural Statistics Service and Economic Research Service of the USDA for dairy production during the calendar year 2000. The survey collects data to measure the financial condition and operating characteristics of farm businesses, the cost of producing agricultural commodities, and information on technology use and management practices.\n",
      "Unfortunately, prices of inputs were not collected and thus it was not possible to estimate a standard cost function where cost is a function of input prices. Rather, cost per hundredweight of milk produced will be estimated as a function of farm characteristics and practices, which we will refer to as a cost equation.\n",
      "The target population for the survey was farms milking 10 or more cows in the 22 major dairy states. The sample is a multi-frame, probability-based survey in which farms were randomly selected from groups of dairy farm\n",
      "i=27, j=199, k=172\n",
      "a=IMPOSSIBLE\n",
      "g=Agricultural Resource Management Survey\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=chonogor, 2011) . Ahmad and Che-Lah (2012) observed that students experience conceptual difficulties as a result of the way they are taught in the classroom, which is predominantly the lecture (traditional) method as well as problem-solving difficulties they experience. Because of this, the majority of learners perform poorly and find it extremely difficult to attain a pass mark of 30% on the chemistry section of the NSC (National Senior Certificate) examinations. Although some stakeholders in education such as DBEMP (Department of Basic Education Mpumalanga Province) continue to support Physical Sciences learners through various interventions, results in Physical Sciences, Chemistry, and specifically Electrochemistry have been declining since 2010 (DBEMP, 2018) except for 2012. Some of the programmes organized by the DBEMP to improve the performance of Grade 12 students in particular in Physical Sciences include winter schools, spring schools, as well as special weekend camps, where expert teachers are brought together to teach students. Table 1 shows the pass percentage of students from 2010-2015 in Mpumalanga Province (DBEMP, 2018 (DBEMP, , 2017 (DBEMP, , 2016 (DBEMP, , 2015 (DBEMP, , 2014 (DBEMP, , 2013 (DBEMP, , 2012 (DBEMP, , 2011 .\n",
      "The pass percentage at 30% shows a continuous improvement from 2010 to 2013 and then a decline in 2014 across the districts and the province, with the Bohlabela district scoring lower percentages. In 2015 there was an improvement in the district performance, which culminated in an improvement in the provincial performance as well. Similarly, the pass percentage at 30% shows a continuous improvement from 2010 to 2012 and then a decline in 2013 and 2014 in Electrochemistry and consequently Chemistry, but the Electrochemistry average is generally lower than the overall average for Chemistry so it is contributing to pull the Chemistry average down. However, in 2015 there was an improvement in the Electrochemistry performance, which led\n",
      "i=28, j=498, k=11\n",
      "a=IMPOSSIBLE\n",
      "g=Trends in International Mathematics and Science Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ies, and other regions, including CA2, 3, 4, and dentate gyrus, have also been implicated in early disease [29] .\n",
      "The current study extends our previous work with whole hippocampal volumes [12] . In addition to replicating prior whole hippocampus findings with advanced segmentation technology and methods for controlling confounding factors [30] [31] [32] , we examined whether sex and diagnosis moderate the effects of A+ on hippocampal SV. Specifically, we hypothesized that NC women with A+ would show CA1 and subiculum volumes comparable to NC women with A-. Although the literature has shown more tenuous support for AD-related changes in CA3, CA4, and dentate gyrus, such findings are nonetheless present, and as such we hypothesized that NC women with A+ would also show volumes in these regions comparable to NC women with A-. We hypothesized that this sex-based hippocampal subfield advantage would be lost in eMCI, with A+ effects being detrimental across sexes.  ADNI is a longitudinal, multi-site AD biomarkers study. This study was conducted in accordance with guidelines on human experimentation and ethical standards of the Committee on Human Experimentation and approved by local Institutional Review Boards at each participating ADNI site. Participants from ADNI2 (NC n = 178; MCI n = 232) and ADNIGO (MCI n = 110) were selected as described previously [11] . The Alzheimer's Disease Neuroimaging Initiative (ADNI) required NC participants to have Mini-Mental State Examination (MMSE) [33] scores of 24-30, Clinical Dementia Rating (CDR) [34] of 0, and no memory complaints. ADNI defined early eMCI as including MMSE score of 24-30, CDR of 0.5, CDR memory box score of 0.5 or greater, objective memory loss as assessed by education-adjusted scores on the Wechsler Memory Scale Logical Memory II test (Raw scores = 9-11 for >16 years education; 8-15 for 5-9 years education; 0-7 for 3-6 years education), subjective memory complaint, and not meeting criteria for dementia [35] . ADNI\n",
      "i=29, j=319, k=381\n",
      "a=11 ]. the alzheimer's disease neuroimaging initiative ( adni ) required nc participants to have mini - mental state examination ( mmse ) [ 33 ] scores of 24 - 30, clinical dementia rating ( cdr ) [ 34 ] of 0, and no memory complaints. adni defined early\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(contexts, questions, truncation=\"only_first\", padding=\"max_length\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "start_logits, end_logits = model(**inputs).values()\n",
    "for i in range(len(start_logits)):    \n",
    "    j = torch.argmax(start_logits[i])  \n",
    "    k = torch.argmax(end_logits[i]) + 1\n",
    "    a = \"IMPOSSIBLE\"\n",
    "    if 0 < j < k:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[i][j:k])\n",
    "        a = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"\\n\\nis_impossible={is_impossible[i]}\\nq={questions[i]}\\nc={contexts[i]}\\ni={i}, j={j}, k={k}\\na={a}\\ng={golds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ce32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
