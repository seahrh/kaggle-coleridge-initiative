{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c7c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6602a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7c08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='pretrained/google/electra-small-discriminator', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "pretrained_dir = \"pretrained/google/electra-small-discriminator\"\n",
    "model_max_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, model_max_length=model_max_length)\n",
    "print(f\"{repr(tokenizer)}\\n{tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75bb4102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained/google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at pretrained/google/electra-small-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"pretrained/google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(pretrained_dir)\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1aee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21600 entries, 191264 to 147282\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Id                    21600 non-null  object\n",
      " 1   is_multi              21600 non-null  int8  \n",
      " 2   ground_truth          21600 non-null  object\n",
      " 3   dataset_labels        21600 non-null  object\n",
      " 4   is_impossible         21600 non-null  int8  \n",
      " 5   answer_start          21600 non-null  int16 \n",
      " 6   answer_end            21600 non-null  int16 \n",
      " 7   context               21600 non-null  object\n",
      " 8   context_token_length  21600 non-null  int16 \n",
      "dtypes: int16(3), int8(2), object(4)\n",
      "memory usage: 1012.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet(\"input/train.parquet\")\n",
    "train = train.sample(frac=0.06)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34457e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "len=21600\n",
      "CPU times: user 20.5 s, sys: 2.56 s, total: 23.1 s\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"what dataset\"\n",
    "questions = [question] * len(train)\n",
    "enc = tokenizer(list(train[\"context\"]), questions, padding=\"max_length\")\n",
    "print(f\"{repr(enc.keys())}\\nlen={len(enc['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02a2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answer_start, answer_end, ids, is_impossible):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(is_impossible)):\n",
    "        j, k = 0, 0\n",
    "        if is_impossible[i] == 0:\n",
    "            j = encodings.char_to_token(i, answer_start[i])\n",
    "            if j is None:\n",
    "                #offsets = encodings[\"offset_mapping\"][i]\n",
    "                _id = ids[i]\n",
    "                raise ValueError(f\"start pos must not be None. i={i}, id={_id}, answer_start={answer_start[i]}\")  #offsets={offsets}\") \n",
    "            k = encodings.char_to_token(i, answer_end[i] - 1)\n",
    "            if k is None:\n",
    "                raise ValueError(\"end pos must not be None\")\n",
    "            if j > k:\n",
    "                raise ValueError(\"start pos must be less than or equals end pos\")\n",
    "        start_positions.append(j)\n",
    "        end_positions.append(k)\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f7847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3170c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 0 ns, total: 31.2 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "add_token_positions(\n",
    "    enc, \n",
    "    answer_start=list(train[\"answer_start\"]), \n",
    "    answer_end=list(train[\"answer_end\"]),\n",
    "    ids=list(train[\"Id\"]),\n",
    "    is_impossible=list(train[\"is_impossible\"]),\n",
    ")\n",
    "train_ds = MyDataset(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdffb5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del enc, questions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/169 [03:16<9:11:21, 196.91s/it]"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "#lr=5e-4\n",
    "optim = AdamW(model.parameters(), lr=5e-4)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    loss_mean = 0\n",
    "    steps = len(train_loader)\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss_mean += loss / steps\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch={epoch}, loss={loss_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70552291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ffc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"output\")\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.sample(30)\n",
    "questions = [question] * 30\n",
    "contexts = list(df[\"context\"])\n",
    "golds = list(df[\"answer_text\"])\n",
    "is_impossible = list(df[\"is_impossible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca26c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(contexts, questions, truncation=\"only_first\", padding=\"max_length\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "start_logits, end_logits = model(**inputs).values()\n",
    "for i in range(len(start_logits)):    \n",
    "    j = torch.argmax(start_logits[i])  \n",
    "    k = torch.argmax(end_logits[i]) + 1\n",
    "    a = \"IMPOSSIBLE\"\n",
    "    if 0 < j < k:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[i][j:k])\n",
    "        a = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"\\n\\nis_impossible={is_impossible[i]}\\nq={questions[i]}\\nc={contexts[i]}\\ni={i}, j={j}, k={k}\\na={a}\\ng={golds[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
