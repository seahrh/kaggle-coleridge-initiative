{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c7c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "import scml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6602a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()\n",
    "scml.seed_everything()\n",
    "seed = 31\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7c08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='pretrained/google/electra-small-discriminator', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "pretrained_dir = \"pretrained/google/electra-small-discriminator\"\n",
    "model_max_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, model_max_length=model_max_length)\n",
    "print(f\"{repr(tokenizer)}\\n{tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6ade8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained/google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at pretrained/google/electra-small-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"pretrained/google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(pretrained_dir)\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1aee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360000 entries, 0 to 359999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   Id                    360000 non-null  object\n",
      " 1   is_multi              360000 non-null  int8  \n",
      " 2   ground_truth          360000 non-null  object\n",
      " 3   dataset_labels        360000 non-null  object\n",
      " 4   is_impossible         360000 non-null  int8  \n",
      " 5   answer_start          360000 non-null  int16 \n",
      " 6   answer_end            360000 non-null  int16 \n",
      " 7   context               360000 non-null  object\n",
      " 8   context_token_length  360000 non-null  int16 \n",
      "dtypes: int16(3), int8(2), object(4)\n",
      "memory usage: 13.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_parquet(\"input/train.parquet\")\n",
    "#train = train.sample(frac=0.07)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34457e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "len=360000\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"what dataset\"\n",
    "questions = [question] * len(train)\n",
    "enc = tokenizer(list(train[\"context\"]), questions, padding=\"max_length\")\n",
    "print(f\"{repr(enc.keys())}\\nlen={len(enc['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02a2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answer_start, answer_end, ids, is_impossible):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(is_impossible)):\n",
    "        j, k = 0, 0\n",
    "        if is_impossible[i] == 0:\n",
    "            j = encodings.char_to_token(i, answer_start[i])\n",
    "            if j is None:\n",
    "                #offsets = encodings[\"offset_mapping\"][i]\n",
    "                _id = ids[i]\n",
    "                raise ValueError(f\"start pos must not be None. i={i}, id={_id}, answer_start={answer_start[i]}\")  #offsets={offsets}\") \n",
    "            k = encodings.char_to_token(i, answer_end[i] - 1)\n",
    "            if k is None:\n",
    "                raise ValueError(\"end pos must not be None\")\n",
    "            if j > k:\n",
    "                raise ValueError(\"start pos must be less than or equals end pos\")\n",
    "        start_positions.append(j)\n",
    "        end_positions.append(k)\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f7847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3170c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 687 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "add_token_positions(\n",
    "    enc, \n",
    "    answer_start=list(train[\"answer_start\"]), \n",
    "    answer_end=list(train[\"answer_end\"]),\n",
    "    ids=list(train[\"Id\"]),\n",
    "    is_impossible=list(train[\"is_impossible\"]),\n",
    ")\n",
    "train_ds = MyDataset(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e188aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "del enc, questions\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(repr(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "813f9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22500/22500 [2:44:02<00:00,  2.29it/s]\n",
      "  0%|                                                                                                                                                                                                                                    | 0/22500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=0.0814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22500/22500 [2:42:15<00:00,  2.31it/s]\n",
      "  0%|                                                                                                                                                                                                                                    | 0/22500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22500/22500 [2:44:57<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2, loss=0.0655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-4, eps=1e-6)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    loss_mean = 0\n",
    "    steps = len(train_loader)\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss_mean += loss / steps\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch={epoch}, loss={loss_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefc2e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForQuestionAnswering(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70552291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9916f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output\\\\tokenizer_config.json',\n",
       " 'output\\\\special_tokens_map.json',\n",
       " 'output\\\\vocab.txt',\n",
       " 'output\\\\added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d20ffc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"output\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"output\")\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d3247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.sample(30)\n",
    "questions = [question] * 30\n",
    "contexts = list(df[\"context\"])\n",
    "golds = list(df[\"dataset_labels\"])\n",
    "is_impossible = list(df[\"is_impossible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c6b6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=close relationship between the actual one-year stay rate and plans to stay as reported to the Survey of Earned Doctorates. This suggests that these data on intentions available from the Survey of Earned Doctorates should be reviewed by those who are interested in the future of stay rates. The plans to stay reported to the 2007 Survey of Earned Doctorates are at a new high. This appendix provides information about the data and methods used to produce the results described in this report. This project was discussed with staff of the National Opinion Research Center (NORC), the National Science Foundation (NSF), and the Social Security Administration to ensure that the methods chosen would comply with each organization's policy regarding the confidentiality of data on individuals. Data for the report pertain almost exclusively to a set of 118 groups of Ph.D. recipients who received S/E degrees from U. S. universities in 1997, 2002, 2005, and 2006. Our method started with responses to the NSF Survey of Earned Doctorates for the years of interest. This survey is not a sample survey but rather a complete census of new doctorate recipients in the United States, administered at or near the time that they complete their doctorates. Among the questions asked of these persons are country of citizenship, degree field, and post-graduation plans. Answers to these questions were used to define and identify groups for which stay rates were estimated (e.g., temporary residents graduating in 2002 with a degree in computer science). The NORC staff then prepared a data file containing the birth years and Social Security numbers of the persons in each of these groups. All the persons with the traits used to define the group were included, provided that NORC had a Social Security number for them. In total, groups of foreign citizens containing a total of 77,920 persons were identified. If no adjustments were to be made, the stay rate would be the proportion in a group that was recorded by the Social Security Administr\n",
      "i=0, j=19, k=24\n",
      "a=survey of earned doctorates\n",
      "g=NSF Survey of Earned Doctorates|Survey of Doctorate Recipients|Survey of Earned Doctorates\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ted immunoassay platform, LumiPulse, based on chemiluminescence detection, 120 tests/hour, monotest cartridges, and Luminex Corporation has been developing immunoassays for CSF Ab , and t-tau, with assay linearity for Ab1-42 and t-tau of 78 pg/mL to 12541 pg/mL and 36 pg/mL to 2068 pg/mL, respectively, and further details will become available for these two immunoassay systems in the future.\n",
      "analytical processes, and full consideration of confounding factors affecting plasma Ab levels, it is possible that plasma Ab levels may become useful as a biomarker of brain Ab amyloidosis and pharmacodynamics of Ab-targeted therapy in AD. The high heritability of late-onset AD (w80% heritability from twin studies) [109, 110] derived a number of genetic association studies, and APOE 4 allele is the best established genetic risk factor for risk to develop AD. The role of the 4 allele as a modulator of the relationship between plasma Ab and Ab pathology in the brain (Ab amyloid PET) was assessed in ADNI subjects. In APOE 42 but not 41 subjects, there was a positive correlation between plasma Ab 1-40 /Ab 1-42 ratio and [\n",
      "11 C]PiB uptake [111] . Another study demonstrated that the expression pattern of plasma proteins determined by multiplex RBM panel was associated with APOE allelic status [73] . These results suggest that APOE genotype is associated with a unique plasma protein profile irrespective of diagnosis, indicating the importance of APOE genotype on plasma biomarker profiles.\n",
      "Recent GWAS and meta-analysis studies identified and confirmed additional genetic variants associated with AD including CLU, PICALM, CR1, MS4A4A, CD2AP, CD33, EPHA1, BIN1 and ABCA7 and APOE [112] [113] [114] [115] [116] . Some specific single nucleotide variants in novel genes detected by robust sequencing technology (e.g., next-generation sequencing) were significantly associated with the progression of hippocampal atrophy in MCI patients without APOE 4 allele [117, 118] . These genetic data may have pr\n",
      "i=1, j=252, k=254\n",
      "a=adni\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=High School Longitudinal Study of 2009 (HSLS:09), Howard and colleagues (2015) found that students who failed algebra in the eighth grade were similar in terms of mathematics proficiency compared to students who passed lower-level mathematics courses but reported lower mathematics motivation. Although this study provided more information on students who fail algebra and their subsequent motivation, there is still a lack of information about students who repeat algebra in particular. For example, little is known about whether retaking algebra from eighth to ninth grade (the transition to high school) helps mathematics achievement, especially over time. Moreover, although Howard and colleagues' (2015) study documented differential motivational patterns for students who repeat algebra, scant studies have investigated whether these students differ in mathematics achievement or mathematics course enrollment by the end of high school. To help address these gaps in the literature, we used a diverse and national sample of United States high school students and investigated whether students who repeated algebra from eighth to ninth grade differed from students who took algebra for the first time in ninth grade on the following outcomes: (a) advanced placement (AP) mathematics enrollment by twelfth grade, (b) mathematics grade point average, and (c) grade point average in STEM (science,\n",
      "Journal of Urban Mathematics Education Vol. 10, No. 1 57 bility might be especially important for students attempting to pass a gate-keeper course like algebra because of its implications for future mathematics course taking and mathematics motivation (Finkelstein et al., 2012;Howard et al., 2015).\n",
      "Journal of Urban Mathematics Education Vol. 10, No. 1 59 specified) 1.69% (n = 204), Hispanic, race specified 20.51% (n = 3,311), More than One Race 7.74% (n = 1,912), Native Hawaiian/Pacific Islander 0.50% (n = 110), and White 51.85% (n = 11,837). This sample was evenly divided by gender, with 10,8\n",
      "i=2, j=1, k=5\n",
      "a=high school longitudinal study\n",
      "g=High School Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=plication of 8 mm or 12 mm Gaussian smoothing kernels. We found that the local GP model evidence was strongly dominated by the variance of noise model, i.e. with lower residual variance resulting in higher evidence. Note, however that the observed evidence differences do not have the same meaning as in the context of Bayesian model comparisons (see e.g. Penny, 2012) where one compares different models of the same data rather than different models of different data, e.g. from different voxels. Most cortical regions provided slightly smaller noise variances compared to subcortical regions especially the thalamus and also the basal ganglia. These regional differences in the GP models might be due to effects of segmentation, nonlinear normalization, the total explained variance by the covariate space, and the true individual differences of local gray matter volume. There was a further tendency to a slightly smaller amount of noise in fronto-temporal compared to occipito-parietal gray matter regions which might be related to the fact that age-related effects in elderly subject brains are often found to be less emphasized in posterior brain regions Raz and Rodrigue, 2006) .\n",
      "Earlier studies demonstrated that VBM and parametric models afford inference about age-related gray matter volume differences in healthy aging groups (Good et al., 2001; Hutton et al., 2009; Kennedy et al., 2009; Ziegler et al., 2012b ) and brain pathology in single patients (Colliot et al., 2006; Mehta et al., 2003; Muhlau et al., 2009; Salmond et al., 2003; Sehm et al., 2011) . In addition, recent studies also showed the potential of recognition models and multivariate classifiers to decode early stage diagnosis based on brain scans in dementia and especially AD Davatzikos et al., 2009 Davatzikos et al., , 2011 Kloppel et al., 2008; Misra et al., 2009; Westman et al., 2011 Westman et al., , 2012 . Although multivariate decoding models are expected to be powerful, sensitive, and highly accurate, we ar\n",
      "i=3, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ent estimates at the top quantiles ( . ) are markedly different from those at the rest of the sample. The test for equal coefficient estimates across different quantiles developed by Koenker and Bassett (1982) was rejected and the author argued that this is an evidence of two separate segments in agricultural land market in Northern Ireland, where good quality agricultural land are scarce. What is common among all of the studies reviewed above is that, by using quantile regression, they have yielded additional insight into the conditional distribution of the dependent variable that would otherwise have been left unnoticed had they employed the traditional least square approach. Although empirical application of quantile regression in agricultural economics have been limited so far, the findings from the studies we reviewed above assures potential of quantile regression model in estimating the hedonic pricing model of farmland values. The study employs data obtained from the nationwide Agricultural Resource Management Survey (ARMS) from 2006 to 2008, developed by the Economic Research Service and the National Agricultural Statistical Service. The 2006-2008 ARMS surveys provide information about the relationships between farmland values, agricultural production, resources, and the environment as well as about the characteristics and financial conditions of farm households. Data are collected from one operator per farm, the senior farm operator, who makes most of the day-to-day management decisions. We also utilize county-level variables developed by the United States Department of Agriculture: the Amenity Index and its components to represent natural amenity of farmland, average monthly rainfall data between 1960 and 2003 to represent production uncertainty, soil productivity index to measure land quality, the 2000 Population Interaction Zones in Agriculture (PIZA) that captures intensity of population interaction. As in Guiling, et al. (2009), the dataset used in this study is unique in that it supplement\n",
      "i=4, j=178, k=182\n",
      "a=agricultural resource management survey\n",
      "g=Agricultural Resource Management Survey\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=hers in American public schools in relation to teachers' race and/or ethnicity. Yet, studies consistently show the racial and ethnic makeup of teachers in the United States is not reflective of the student population (Villegas, Strom, & Lucas, 2012). For example, in the 2014-2015 school year, White teachers were 80% of public school teachers, but White students were only about 50% of public school students (National Center for Education Statistics, 2018). Proportions of students of color are projected to continue to grow in the coming years, now surpassing the proportion of White, non-Hispanic students (Hussar & Bailey, 2013). There are no indications of a similar shift among teachers. In fact, schools continue to struggle with the recruitment and retention of teachers of color (Ingersoll & May, 2011). Of teachers of color who do enter into the workforce, there is evidence that they are not evenly distributed among all schools. Ingersoll and May (2011) found that in 2003, minority teachers comprised 29% of urban schools (compared to 13% of suburban schools), 35% of schools serving high concentrations of high poverty students (compared to 7% of low poverty schools), and an incredible 42% of schools serving high concentrations of minority students (compared to 2% of low minority schools). These figures are especially notable given that teachers of color comprised only about 17% of the teaching workforce nationally (National Center for Education Statistics, 2018). The inequitable distribution of teachers among majority-minority and majority-White schools is also associated with uneven resource allocation. Less experienced teachers, larger class sizes, and fewer curricular resources consistently plague schools serving students of color (Clotfelter, Ladd, & Vigdor, 2005;Darling-Hammond, 2004;Kozol, 2005). Additionally, systemic inequalities that persist in the United States have resulted in disproportionately high rates of poverty among minority communities. Black and Hi\n",
      "i=5, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Common Core of Data\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=common y-axis in figure 5b. The stacked graph has each sensor depth shown separately and in sequence of their positions in the soil profile. The beginning values of soil water content at each depth are shown on the y-axis of the stacked graph ( fig. 5a ). The scale of the graphs at individual depths is different, but the graph is easier to interpret for irrigation management than the common axis graphs shown in figure 5b because the individual graphs are kept separate. Eight pivot irrigations were applied during this 12-day example. Soilwater peaks are clearly visible at shallow depths, as is the subsequent reduction in soil water content caused by drainage and ET. Four of the eight irrigation events resulted in water percolation to the 100-cm depth (indicated by arrows in fig. 5a ), showing that the farmer was applying too much water, i.e., water percolation below daily water uptake depths (discussed below). Note that this deep percolation occurred only when there were irrigation events on two consecutive days. Total deep percolations to at least 100 cm, across all crops and irrigation events over two years are shown in table 1.\n",
      "Drip irrigations had nearly 43% more irrigation events that percolated to 100-cm as compared to pivot-irrigation (30% vs. 21%). This higher deep percolation rate under drip irrigation was due in part to the point source of water input and the placement of the MCPs within about 6 cm of the drip tape, in contrast to the more spatially uniform distribution of water under pivot irrigation. This calculation may underestimate the proportionate number of irrigation events with water percolation to 100-cm depth for both irrigation methods because the soil was often already near saturation at that depth in some of the fields. Under such conditions, water that percolates into near-saturated soil may not result in a detectable increase in soil water content (Timlin et al., 2001; Starr and Timlin, 2004) . The percolation of water through the soil profi\n",
      "i=6, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Census of Agriculture\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=her research group showed an increased gray matter volume in the temporal pole and the anterior cingulate cortex in subjects with a risk allele in the CNNM2 [51] . The same group also reported no association of the risk SNP in CSMD1 and gray matter volume [52] . Collaborative genome-wide association analysis supports a role for calcium channel, voltagedependent, L type, alpha 1C subunit (CACNA1C) and ankyrin 3 (ANK3) in bipolar disorder [53] . Healthy subjects with risk allele carriers showed significantly increased gray matter volume in a corticolimbic frontotemporal neural system compared to those without a risk allele [54] . A risk allele of ANK3 was associated with cortical thinning in patients with firstepisode psychosis [55] . However, Tesli et al. reported no association between the brain structural measures found in bipolar disorder, including gray matter volumes, and nine SNPs in the genes CACNA1C, ANK3, odd Oz/ten-m homolog 4 (ODZ4) and spectrin repeat containing nuclear envelope 1 (SYNE1) [56] .\n",
      "Overall, the effects of the risk SNPs in the risk genes identified by recent GWASs on brain structure have not been consistent. This inconsistency may be due to the very small number of studies reported for each gene/SNP, the small sample sizes, and differences in the methodology of brain morphological measurements and in the tested anatomical regions. Therefore, further replication studies using larger sample sizes are necessary to draw conclusions. Recently, several genome-wide association studies using neuroimaging as a phenotype have been reported. These studies sought new genetic variations to explain the risk for neuropsychiatric disorders and the underlying mechanisms of brain structure and function. A larger sample size compared to that of a candidate gene approach can be assumed because the significant p value threshold is very strict (<5.0 x 10-8) in GWAS.\n",
      "In the early stage of GWAS brain morphology, the Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "i=7, j=440, k=453\n",
      "a=alzheimer's disease neuroimaging initiative ( adni )\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=or the Advancement of Science [AAAS] 1993; Australian Curriculum, Assessment and Reporting Authority [ACARA] 2013) and is commonly defined as the capacity to use scientific knowledge, to identify questions and to draw evidence-based conclusions in order to understand and help make decisions about the natural world and the changes made to it through human activity (OECD 2013). In recent times, science education scholars have extended their conceptualisation of scientific literacy to focus on the 'literacy' component of the construct (Fang and Wei 2010;Hand et al. 2003;Johnson and Zabrucky 2011;Norris and Phillips 2003). These scholars claim that student conceptualisations of science are developed via oral and written communication, thus highlighting the central role of literacy skills in science education. Fang and Wei (2010) argue that limited reading skills hamper the development of deep understanding of science concepts. In addition, the manner in which textbooks are written influences the development of students' scientific literacy, as students are required to negotiate meaning from the written text, which requires an understanding of the structure and organisation of expository texts (Penney et al. 2003). Thus, science textbooks have a significant role to play in science classrooms, which has implications for their selection and use (Ford 2004). The previous review has drawn attention to the importance of providing engaging learning experiences in junior secondary science classes to tempt more students into post-compulsory science courses. Rightly or wrongly, research shows that the majority of science classes use textbooks, and in many cases, the textbook becomes the enacted curriculum. As such, the selection and use of high quality textbooks is imperative to maximise effective learning outcomes in the science classroom. Very few empirical studies on textbooks have been conducted in Australia (e.g. Ninnes 2000;Wilkinson 1999). In a related set of studies in Qu\n",
      "i=8, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Trends in International Mathematics and Science Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=Agricultural Resource Management Survey from USDA/ ERS. Figure 1 shows violin plots of total value of farm sales for organic producers compared to the non-organic (conventional) producers. A quantile approach is justified based on the significant degree of heterogeneity in the earnings distribution with groups of farmers clustering in upper and lower tails. The remainder of the paper is organized as follows. In the next section, we discuss empirical framework and econometric model. Then we provide discussion about the data used in this study and present our results. The last section summarizes our findings and concludes. Let be a total value of farm sales (earnings) for farm household i. Then determinants of are represented by following equation: Combining equation (1) and(2), we obtain: We assume that the unobserved characteristics are uncorrelated with the participation in organic production. Under this assumption, equation 3can be estimated using a class of estimators such as ordinary least squares (OLS) or quantile regressions. Next, we present the distinct advantage of using unconditional quantile regression (UQR) approach over OLS. In this section, we provide discussion about our econometric approach. We motivate our approach by documenting a significant degree of heterogeneity in farm earnings. Figure 1 shows violin plots of farm earnings by participants and non-participants of organic farming. Violin plots combine box plots and density traces in one diagram so that we can see the center, spread, asymmetry, and distribution of data-peaks and bumps in one place. Violin plots show that the largest share of producers with organic sales cluster at lower and mid-levels of total farm sales. If we use ordinary least squares (OLS) estimators, we cannot capture such heterogeneity because OLS are based on location shift models (Heckman, 1979) which estimate conditional means of dependent variable (y) given independent variable (x)-with a very restrictive assumption tha\n",
      "i=9, j=1, k=5\n",
      "a=agricultural resource management survey\n",
      "g=Agricultural Resource Management Survey\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=g., working memory, inhibitory control, cognitive flexibility) constitute comparatively more promising targets of interventions designed to prevent or remediate learning difficulties. Theoretically, working memory deficits might constitute especially strong impediments to young children's academic achievement, particularly in mathematics (Toll et al., 2011). This is because many classroom tasks given in the primary grades (e.g., counting, solving single-digit addition and subtraction problems) often require children to store as well as manipulate information. In contrast, other types of executive functioning deficits, including in cognitive flexibility, may not be as important during the primary grades. For example, relatively fewer classroom tasks during these grades require solving multi-step problems, thereby placing fewer demands on children with cognitive flexibility deficits (van der Sluis, de Jong, & van der Leij, 2004). Yet whether working memory or cognitive flexibility deficits should receive greater emphasis in early intervention designs is presently unclear because of the lack of studies that have simultaneously examined the predictive utility of both types of deficits in explaining children's risk for learning difficulties (Jacob & Parkinson, 2015). Additionally, and although prior studies have examined whether and to what extent executive functions are related generally to children's achievement in reading or mathematics, less clear is whether specific executive functioning deficits are related to learning difficulties in both reading mathematics, particularly after accounting for the strong potential confounds of children's background characteristics (Jacob & Parkinson, 2015). Thus, it remains to be empirically established whether these deficits are more likely to constrain children's mathematics than reading achievement. Relatedly, finding that executive functioning deficits increase children's likelihood of experiencing learning difficulties in both\n",
      "i=10, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Early Childhood Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=991) Child Behavior Checklist (CBCL), and use of services, a disturbing finding has been that some predictors of use are nonclinical factors such as being older and being White (Garland & Besinger, 1997; Horowitz, Putnam, Noll, & Trickett, 1997; McMillen et al., 2004) .\n",
      "Placement in out-of-home care is a major factor in service use, with children living in foster or kinship care being more likely to receive services than children who stay with their biological parents . For example, one study using NSCAW reported that more than half (57%) of children aged 2 to 14 years and in foster care received mental health services . A second NSCAW study (Stahmer et al., 2005) of young children in out-of-home care found that 29.2% of 3-to 5-year-olds received mental health services, and 37.0% of 3-to 5-year-olds received special education services. Not surprisingly, the percentages were smaller for children 0 to 2 years, among whom 9.1% received mental health services and 13.3% received special education services (Stahmer et al., 2005) .\n",
      "The goal for children in out-of-home care who cannot return home is adoption, but no study specifically has compared rates of service use for children who were adopted with rates for children who remained in foster or kinship care. The mental health needs and services of children adopted from the CWS deserve special attention because their preadoption experience includes both maltreatment and dislocation, which have been associated with emotional, behavioral, and special education needs (Berry & Barth, 1989; Brodzinsky, 1993; Brodzinsky, Radice, Huffman, & Merkler, 1987; Ornelas, Silverstein, & Tan, 2007) . An analysis of a statewide sample of adopted youth aged 4 to 18 years found that high levels of externalizing behavioral problems were associated with preadoption abuse and neglect, prenatal drug exposure, and multiple placements (Simmel, Brooks, Barth, & Hinshaw, 2001) . Further analysis of the needs and service use of adopted children from \n",
      "i=11, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Education Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=Census of Agriculture sample frame, which is the most comprehensive and up-todate list of U.S. farmers. The sample frame included only farm operations with greater than 80 acres of corn production and a minimum of US$100,000 of gross sales. Across the 11 states, farm operations with 2007 gross sales of at least US$100,000 represent 27 % of farms with cropland and 78 % of all cropland acres (USDA-NASS 2009).\n",
      "The sample was stratified by 22 six-digit Hydrologic Unit Code (HUC) watersheds. The HUC system is a standardized watershed classification system developed by the U.S. Geological Survey that organizes watersheds in a nested hierarchy by size (USGS 2012). The sample was stratified by watershed because: (1) climate and ecological conditions that vary spatially are expected to influence farmer perspectives on climate change; (2) ecological conditions vary largely by hydrological unit and shape agricultural systems; and, (3) many of the predicted impacts of climate change are hydrology-related. The 22 watersheds are contiguous and cover substantial portions of Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, Ohio, South Dakota, and Wisconsin (see Figure 1 in the supplemental materials). For this paper, we present statistics for the region as a whole.\n",
      "The survey was mailed in February 2012 to 18,707 eligible farmers using a three-wave mailing process: first a survey was mailed, then a postcard reminder, then a final survey to non-responders. Completed surveys were received from 4,778 farmers for an effective response rate of 26 %. Statistical tests for non-response bias at the watershed level detected no meaningful differences between respondents and non-respondents, indicating that our sample is representative of the target population. Because our random sample of farmers is stratified by watershed, it was necessary to account for differences in response probability between watersheds prior to calculating statistics for the region as a whole.\n",
      "i=12, j=1, k=4\n",
      "a=census of agriculture\n",
      "g=Census of Agriculture|NASS Census of Agriculture\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=ed for a much smaller percentage, accounting for only 15 percent of acres farmed and 13 percent of the cropland.\n",
      "The importance of land ownership for retirement farms was also revealed by average farm net worth of more than $1 million. This was the third highest among the typology groups. Small business-focused farm households had the lowest average net worth for farms and for households. This group had average acres owned that was similar too lifestyle farms at around 150 acres. Farm operators reported in the 2000 ARMS how important various goals were in regards to the agricultural operation. Farm operators tended to weight one or a few goals most heavily, while discounting the rest as essentially unimportant. The goals considered most important to individual operators varied, however. The share of farms that identified certain goals as unimportant or very unimportant demonstrates some of the variation across farm typology groups. For example, more than half of the retirement farms indicated that goals associated with businesses expansion were unimportant. Two groups that also did not view expansion as important were small business-focused farms and low-income farms (28 percent). Lifestyle and small businesses-focused farms had the highest share of operators that indicated the goal of providing an adequate income to the household without having to work off the farm was unimportant. The commitment to off-farm work by farmers in these two groups was also consistent with the amount of off-farm hours and the share of operators working for wages and salaries off the farm. For lifestyle farms, 93 percent of the operators worked off farm -averaging more than 1,900 hours a year. One measure of the importance of government payments to farm households is the amount of payments relative to all sources of gross cash farm income. Appendix Table 3 shows farm and household characteristics for different levels of government payments (excluding conservation programs) divided by gro\n",
      "i=13, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Agricultural Resource Management Survey|Census of Agriculture\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=l inclusion and exclusion criteria were as follows for normal subjects: Mini-Mental State Examination (MMSE; Folstein, Folstein, & McHugh, 1975) scores between 24 and 30 (inclusive), Clinical Dementia Rating (CDR; Morris, 1993) of zero, nondepressed, non-MCI, and nondemented. In addition, CDR sum of boxes (CDR-SB) was calculated as a measure of clinical functioning (does not need to be zero). The subject pool was further restricted to those subjects for whom adequate processed and quality checked MR data were available by February 2009. One hundred eighteen healthy elderly had been tested at Visits 1 (baseline), 3 (1 year), and 5 (2 years), and had MR of sufficient quality. MR change was measured from Visit 1 to Visit 3 (1-year change). To be included in the analyses, CDR-SD and MMSE needed to remain identical or improve over 2 years. In addition, raw scores on each of the following seven neuropsychological tests had to remain at 90% of initial score or higher: Auditory Verbal Learning Test (AVLT) learning (hits -false alarms), AVLT 30min delayed recall (hits -false alarms; Rey, 1941) , digit span (sum of forward and backward), clock copying (Goodglass & Kaplan, 1983) , digit symbol substitution test (Wechsler, 1981) , and logical memory test immediate recall and logical memory test delayed recall (Wechsler, 1987) . This means, for instance, that a participant who scored 10 points on the AVLT delayed recall test at baseline and 8 points after 2 years would be excluded, whereas a participant who scored 9 points after 2 years could be included. After applying these very stringent exclusion criteria, 21 participants were included in the longitudinal analyses (mean age at baseline  76.8 years, range  63.0 -90.2 years; 6 women and 15 men).\n",
      "MR acquisition and analysis. All scans used for the present analyses were from 1.5 T scanners. Data were collected across a variety of scanners with protocols individualized for each scanner, as defined at http://www.loni.ucla.edu/ADNI\n",
      "i=14, j=466, k=468\n",
      "a=adni\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c= to include a broader leadership 11 role in the school, and that such efforts resulted in benefits for the school and the students 12 (Centeio, Erwin & Castelli, 2014). However, a key element of that study was the conclusion that 13 teachers continue to focus on quality PE first, and then on implementing and sustaining other 14 CSPAP components, as they are suitable within the context of each school.\n",
      "The results regarding CE in this study are novel in several ways. First, these data provide 1 new information about how much CE in-service teachers are receiving, specifically on PE-2 related topics. Often, due to the organizational structure of educational agencies and a lack of 3 economies of scale for providing PE-specific professional development for multiple teachers, CE 4 opportunities may not be PE-specific, yet it is one of the crucial elements of effective 5 professional development for in-service PE teachers is that it should be based within a 6 community of physical educators (Armour & Yelling, 2004). We found that 50.4% of schools 7 required CE on PE-related topics. This is lower than earlier estimates also using a national 8 survey in 2009-12, where 69.7% of schools required CE on PE topics. Although we used the 9 same sampling approach (i.e., mailback surveys of a nationally-representative sample), the item 10 wording was different. Previously, researchers had asked whether newly hired PE teachers are 11 required to earn CE credits on PE topics (Author citation, 2014). In the current study, the item 12 pertained to all PE teachers, and was embedded within a set of items about PE staffing and 13 resources; it is possible that the PE teacher was more involved in helping with this survey, 14 resulting in more-accurate responses. Furthermore, respondents at 12.5% of these schools 15 indicated that they did not know the answer to the item about CE-PE. Because most respondents 16 were principals who should-presumably-know about the supports available to and expe\n",
      "i=15, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Common Core of Data\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=ce between the county or HUC-8 total population and the public-supply population served for the same geographic area types. The self-supplied domestic withdrawals are computed using the selfsupplied domestic population and the per capita use coefficients derived from the public-supply deliveries to domestic users. Data from State agencies (water resource departments, permitting offices, State engineer offices), conservation boards, water authorities, Federal agencies, water commissions, water districts, and a database purchased by the USGS NWUIP (Hoover, by Dun and Bradstreet; http://www.dnb. com/products/marketing-sales/dnb-hoovers.html) which includes industrial facility listings with ancillary information on the number of employees, produced commodities, and locations. Some facilities are contacted directly for verification. Estimation methods (to fill in data gaps from reported data) include using employee populations and water-use coefficients on the basis of the commodity production at facilities. Data sources include State agencies (water resource departments, State engineer offices, departments of health) and urban water management plans. Within the Colorado River Basin, only selected counties and areas were completed for some States. Estimation methods include using coefficients-ofuse based on ancillary data such as size of community or entity. Data sources include State agencies (water resource departments, State engineer offices), USDA Census, and NWUIP datasets (Lovelace, 2009a) , as well as metered withdrawal data. Estimates are based on livestock demand coefficients and livestock population if water withdrawal is not metered. Data sources include State agencies (water resource departments, State engineer offices), water conservation boards; oil and gas conservation commissions; bureaus of mines and geology; departments of business and industry; departments of water rights; and divisions of oil, gas, and mining facilities that monitor and report water u\n",
      "i=16, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Census of Agriculture\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=at the regression errors have constant variance, are uncorrelated with each other in time, and have a normal distribution. Analyses were performed using SAS statistical analysis software. [12] The first goal of this study is to resolve the spatial variability of NH 3 emissions from agricultural sources in the southeast United States. To develop this regional emissions inventory, emission estimates from all major agricultural sources in the southeast United States were calculated using data from 1997. The sources considered in this inventory include dairy and beef cattle, poultry, swine, horses, and sheep, as well as fertilizer application. County totals are obtained for each source, and NH 3 emission estimates are performed at the county level. For the purpose of this study, NH 3 emissions are assumed to be uniform across the county. This provides a general spatial distribution of estimated NH 3 emissions across the eight-state region. County-level NH 3 emission estimates for each source type are based on the following equation:\n",
      "The activity data is simply the number of animals present in each county, and is obtained from the 1997 U. spheric temperature and humidity, waste-handling practices, and many other parameters [Asman, 1992] . Because of the many uncertainties, it may be difficult to obtain an accurate NH 3 emission estimate. Table 3 is a summary of documented emission factors that were considered in developing the emission inventory for this study. The large variation in estimates illustrates the difficulty in developing precise estimates.\n",
      "[13] Emission factors were selected for each livestock group including beef and dairy cattle, hogs and pigs, chickens, broilers, turkeys, horses, and sheep. An earlier study and literature review by Battye et al. [1994] refined European emission factors based on United States agricultural practices. Their results have been used as a guide to obtain the emission factors employed in this study. The U.S. Census of Agriculture\n",
      "i=17, j=379, k=382\n",
      "a=census of agriculture\n",
      "g=Census of Agriculture\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=MSE and ADAS-Cog) at 24-month by using the multimodality data at previous time points (i.e., baseline, 6-month, 12-month and 18-month). In our second set of experiments, we predict the conversion of MCI by using the multimodality data at time points which are at least 6-month ahead of the conversion. Our hypothesis is that the proposed pattern analysis method based on both baseline and longitudinal multimodality data would perform better in predicting the future changes of MCI patients than the conventional methods. The data used in the preparation of this paper were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (www.loni.ucla.edu/ADNI). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies, and non-profit organizations, as a $60 million, 5-year public-private partnership. The primary goal of ADNI has been to test whether the serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials.\n",
      "The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of CaliforniaSan Francisco. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55 to 90, to participate in the research, approximately 200 cognitively normal older individuals to be followed for 3 years, 400 people with MCI to be followed for 3 years and 200 peop\n",
      "i=18, j=144, k=146\n",
      "a=adni\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=n of association across levels of schooling emerges among men. The influence of parental income on sons' income level is substantial among those without a college degree, it declines to insignificance among college graduates, and then it regains strength among advanced-degree holders, to levels even higher than for those with less than high school (again, PSID sample sizes are small, so a note of caution is warranted). Remarkably, this pattern is similar across data sets, providing reassurance against sample idiosyncrasies. Among women, the U-shaped pattern is less pronounced but still visible in the NLSY79, whereas there appears not to be substantial variation in mobility across levels of schooling according to the PSID. Together with the weak intergenerational earnings correlation among women (also obtained from the PSID), this is the only instance in which findings depart from a U-shaped pattern across levels of schooling. The overall Do the findings of virtually null intergenerational association among BA holders but pronounced intergenerational reproduction among those with an advanced degree identify change or stability over time? This question cannot be addressed by the current analysis given that all previous studies collapse these groups into a single \"college graduate\" category. In order to ascertain mobility trends, I examine the intergenerational socioeconomic association separately for BA holders and advanced-degree holders among adults during the 1970s and 1980s and use this analysis as baseline for comparison for current findings. I pool GSS surveys from 1972 to 1983 to examine class and occupational status mobility and use a merged sample from the NLS Original Cohorts data set to study mobility of occupational status and total family income. The findings, reported in table 6, are unambiguous. In both samples, and for all measures of socioeconomic standing, the intergenerational association among both BA holders and advanced-degree holders is not sign\n",
      "i=19, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Baccalaureate and Beyond\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c= deficits and effort, reviews the results, and summarizes impairment in each of five clinical domains (orientation, attention, memory, language and perception) as probable, possible or not present and renders an opinion regarding the presence of dementia and AD. For neuropathology, JAS is one of three board-certified neuropathologists involved in reviewing and confirming postmortem assessments. Microglia counts were obtained by trained research assistants and postdoctoral fellows -inter-rater reliability is excellent. For all protocols, neuropathological examiners were blind to all clinical data including but not limited to age, gender, education, and diagnosis at time of death. In ADNI, RBM was blinded to the clinical status of subjects who contributed plasma samples. For ROS/MAP, all available data on each phenotype of interest were extracted by a trained database manager and transferred securely to CAMH for statistical analyses. For ADNI, data were downloaded directly from the LONI IDA with institutional permission. In vivo cerebral infarcts and white matter hyperintensities (ADNI and ROS/MAP) Table 1 summarizes demographic characteristics of the ADNI 1 and ADNI GO and 2 cohorts analyzed, according to TSPO rs6971 genotype.\n",
      "In the ADNI 1, GO, and 2 combined Caucasian sample (n 14 1151), rs6971 genotype was not associated with the presence of cerebral infarcts (Wald 2 1 14 1.98, P raw 14 0.16) ( Figure 1 ). However, there was a nominal association with infarcts in the LMCI group (Wald 2 1 14 3.98, P raw 14 0.047), whereby likelihood of having infarcts increased stepwise with decreasing binding affinity (O.R. LAB:HAB 14 2.97, C.I. 95% 14 (1.01,8.70)). No associations were observed in the CN (Wald 2 1 14 2.67, P raw 14 0.10), EMCI (Wald 2 1 14 0.01, P raw 14 0.94), or AD group (Wald 2 1 14 1.13, P raw 14 0.29), though there were no AD subjects who had both infarcts and the\n",
      "i=20, j=138, k=140\n",
      "a=adni\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ADNI cohort used different genotyping platforms. Before the imputation, we performed standard sample and SNP quality control procedures as described previously [35] . Furthermore, in order to prevent spurious association due to population stratification, we selected only non-Hispanic Caucasian participants that clustered with HapMap CEU or TSI populations using multidimensional scaling analysis (http://www.hapmap.org). Imputation was performed using MACH and minimac in a two-stage procedure as described previously [36] . The pilot 1 data of the 1000 Genomes Project were used as a reference panels for inferring missing genotypes. Minimac produced the posterior probabilities of the imputed genotypes at un-genotyped marker loci for each individual. In order to assess the quality of imputation, an r 2 value equal to 0.30 was imposed as the threshold to accept the imputed genotypes. T 1 -weighted brain MRI scans at baseline were acquired using a sagittal 3D MP-RAGE sequence following the ADNI MRI protocol [37] . As detailed in previous studies [38] , a widely employed automated MRI analysis technique was used to process MRI scans: FreeSurfer V5.1 software (http://surfer.nmr.mgh.harvard.edu/). FreeSurfer was used to process and extract brain-wide target MRI imaging phenotypes (region volume and cortical thickness) by automated segmentation and parcellation. The cortical surface was reconstructed to measure thickness at each vertex on the surface. The cortical thickness was calculated by taking the Euclidean distance between the grey/white boundary and the grey/ cerebrospinal fluid (CSF) boundary at each vertex on surface. For surface-based comparison of the cortical thickness, all individual cortical surfaces were registered to a common surface template, which was an average created from all cognitively normal control subjects. The cortical thickness was smoothed with 10 mm FWHM Gaussian kernel to improve the signalto-noise ratio and statistical power. The common surface \n",
      "i=21, j=1, k=3\n",
      "a=adni\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=ADNI dataset of 54 subjects through ninefold cross-validation and compared between the preliminary segmentation step (MRI U-Net) and the additional steps using only cropped data (Cropped MRI U-Net) or cropped data together with shape context (Shape MRI U-Net), as shown in Table 2 . No relevant differences were found between the three methods, which showed quite consistent results on both the left and the right hippocampus. Among all the tested DL-based methods, Tissue MRI U-Net showed the worst performance, having a slightly lower accuracy and higher Hausdorff distance in average compared to the other methods.\n",
      "The average Dice score was also estimated within each of the three diagnostic groups (HC, AD, and MCI). This was done to check whether the system has a consistent performance across all possible forms of hippocampal integrity. As shown in Figure 2 , all diagnostic groups showed a similar segmentation accuracy in both the left and right hippocampus by using the three proposed methods. However, the AD patients always presented a slightly lower Dice score (1 or 2% lower in average) with respect to the other two subject groups.\n",
      "As presented in Table 2 , our methods yielded better values than FreeSurfer in all the considered evaluation metrics. This applies also for the comparison between diagnostic groups (see Figure 2) , in which, contrary to the proposed DL methods, FreeSurfer showed a higher performance loss when dealing with MCI and-even more-AD subjects, compared to the HCs.\n",
      "In order to better understand the influence of each of the three independent U-Nets (one for each view) toward the final segmentation, we also computed the evaluation metrics separately for each U-Net (see Supplementary Table S1 ). These results showed how, for MRI U-Net, the highest accuracy is obtained on the axial view. By contrast, for Cropped MRI U-Net and Shape MRI U-Net, the highest accuracy can be observed on the coronal view. However, while no big differences can be found across \n",
      "i=22, j=1, k=3\n",
      "a=adni\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=action with graduates they hire. They said that they would be willing to complete this type of survey if it would provide them with benefits such as access to aggregated information about graduates' institutions of postsecondary education attended, degrees, or starting pay. However, one association official stated that access to this information might not be enough of an incentive to compel employers to complete the online survey. graduates and has information categorized by major and institution. 29 One major advantage of surveys is that because students themselves provide the information, FERPA compliance is not an issue. However, existing surveys have limitations. For example, surveys that are able to bridge postsecondary education and employment, like the Baccalaureate and Beyond Longitudinal Study, are compiled infrequently: That study has followed groups of students who graduated in 1993 and 2000, and data collection is under way for a third group of 2008 graduates. Further, the Baccalaureate and Beyond Longitudinal Study is representative for graduating seniors nationally and across all majors, but is not representative of any given state or institution, precluding analyses at those levels. 30 A few stakeholders also said that because surveys rely on self-reported information, they might be less reliable than other data sources. Other stakeholders noted that surveys sometimes have low response rates, and results might have significant lag time between data collection and data publication, and incur costs each time a survey is administered. (See table 2 for a summary of the various approaches.) 29 The National Association of Colleges and Employers Salary Survey compiles data from career planning and placement offices of colleges and universities across the United States. The reports consist of starting salary offers made to new graduates by employing organizations in business, industry, and government, and by nonprofit and educational institutions. The figures reported are for base salaries only and do\n",
      "i=23, j=140, k=143\n",
      "a=baccalaureate and beyond\n",
      "g=Baccalaureate and Beyond|Baccalaureate and Beyond Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=ampus climate; inadequate commitment to educational goals and the institution especially as regards the role of the university vision and mission in meeting the student academic and social goals; issue of social and academic integration of students with developmental and learning disabilities (How committed are the University, Student Government Board, and Student Center Programs to attaining the goals?); accessing financial aid by the students from low income and minority groups.\n",
      "There is no doubt that the faculty and administrative staff of every higher institution is equipped with innovative tools and resources to conduct student-centered instruction and provide support services. They possess different expertise in a variety of areas, with strong teaching background and professional service profiles to face the challenges in the classroom. This is evident from their rigorous years of training and research in different fields.\n",
      "Furthermore, these instructors, lecturers or professors possesses skills in large class instruction, use of classroom technology, knowledge of research methodology, planning and implementation of service learning projects, course-embedded assessment, engaging students in learning, as well as interpersonal skills which benefit and enhances students' learning, research and academic achievement. Although American colleges and universities utilize the above mentioned teaching-learning facilities, they still experience exceptional student attrition. This paper examined the reasons behind the student departure from the classroom and campus environment, aside from family or financial problems.\n",
      "It is pertinent to note that several students with developmental and learning disabilities have behavioral issues and pressing academic problems -some are displayed in the classroom, others could be noticed when they fill student evaluation forms at the end of the semester. Among the signs in the classroom which could be a signal to the instructor or professo\n",
      "i=24, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Beginning Postsecondary Student\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c=ity purposes, the study estimated z scores of individual states' proficiency standard cut scores based on the normal distribution table and then averaged z scores across states. 4 Once all of the state, national, and international standards of math proficiency were identified by within-grade z scores, these scores were then converted into cross-grade standard scores (within-grade z scores + national average g scores).\n",
      "There are commonalities between NAEP and TIMSS that warrant linking the assessments for comparison of performance standards. First, both assessments were based on similar curricular frameworks; although the two assessments have no common items, content analyses of both assessments suggest similarities that sufficiently warrant linkage for global comparisons. NAEP and TIMSS assessments are sufficiently similar to warrant linkage for global comparisons but not necessarily good for detailed comparisons of areas of student achievement in subtopics (McLaughlin, , 1997; NCES, 2006) . Second, both assessments were administered to nationally representative samples of students at the same grade in the same year (Grades 4 and 8 in 2007) . A similar kind of linkage was made between NAEP and state assessments, because previous studies showed comparability of the two assessments in terms of content despite discrepancies in the rigor of performance standards (NCES, 2007) . Table 1 summarizes the results of logistic regression analysis including parameter estimates and model fit statistics by data set and grade. Using math achievement alone as a predictor explained about a 30% to 60% range of variance in the chance of being on track to college readiness (see column 3 in Table 1 ). The shorter the time gap between college outcomes and math achievement, the stronger the predictive validity; the end of high school math achievement at Grade 12 had slightly higher predictive validity than math achievement at 10th grade or 8th grade in terms of R 2 values, for all levels o\n",
      "i=25, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Early Childhood Longitudinal Study|Education Longitudinal Study|National Education Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=test is not performed and the significance of GE interactions for all markers retained after screening is assessed using Wald tests with FDR correction (according to the method of Benjamini and Hochberg (1995) For all four SPUR variations, score statistic-based prescreening, as detailed in Section 3.1.1, was performed using = 20 , where q is set to be the same for all SPUR methods and for the two screening-testing benchmark approaches. The SPUR method and all benchmark approaches were implemented in R (R Core Team, 2014). The first two stages together filter the set of hypotheses using statistics that are independent of the logistic regression-based  test statistics computed in the last two stages under 0 . As detailed in Section 2.3, the fact that filtering is performed using filter statistics that are independent of the test statistics under 0 ensures that FDR control can be maintained on just the family of hypotheses that pass the filter(s). Thus, the SPUR method can provide FDR control on just a single level of postfiltering hypotheses using standard FDR methods (Benjamini & Hochberg, 1995 ) (e.g., the family of two omnibus tests for the test-stage models based on the marginal association and gene-environment correlation filters) or on a hierarchy of postfiltering hypotheses using hierarchical FDR methods (Yekutieli, 2008 ) (e.g., the hierarchy formed by omnibus tests at the top level followed by individual interaction tests). To evaluate the extended SPUR method, we analyzed a combination of simulated and real genotype data sets. The first simulation study used a simple framework similar to that employed by Dai et al. (2012) to evaluate type I error control and power. The second simulation study also assessed type I error control and power and was based on the disease architecture models developed by Aschard et al. (2012a) for breast cancer, type 2 diabetes, and rheumatoid arthritis. Finally, GWAS data from the Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "i=26, j=425, k=438\n",
      "a=alzheimer's disease neuroimaging initiative ( adni )\n",
      "g=ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\n",
      "\n",
      "\n",
      "is_impossible=1\n",
      "q=what dataset\n",
      "c= realistic standards: foster a sense of realism about appropriate achievement levels. Unrealistic standards contribute to lower performance levels. 4. Identifying correlation of achievement with outcomes: correlate a range of factors with outcomes to identify strengths and weaknesses in the system. 5. Directing teachers and raising students' performance levels: draw teaching and learning into focus with national assessment to achieve desired outcomes represented in given indicators. 6. Promoting accountability: use assessment to justify resource allocation. 7. Increasing public awareness: publish national assessments to attract considerable media attention, thus heightening public consciousness on educational matters. 8. Informing political debate: direct statistical evidence to issues arising in an educational systemit is more likely to initiate reform. As a guide, the original aims of the U.S. NAEP in the mid-20th century were both political and technical. Termed \"the Nation's Report Card,\" its aims were to track educational reform, educational research, and the war on poverty (Jennings & Renter, 2006). The aims of national assessment in New Zealand's National Educational Monitoring Project differ in that they are explicit: ...To get a broad picture of the achievements of representative samples of New Zealand school students at successive points in time so that  trends in educational performance can be identified and reported;  good information is available to assist policy makers, curriculum specialists and educators with their planning;  the public can know about trends in educational achievement (New Zealand Ministry of Education, 2009, webpage). National assessment may be viewed as both an instrument for reform and a measure of change. In these guises, assessment methodology is open to public scrutiny from a dual perspective regarding purpose and effectiveness (Whetton, Twist, & Sainsbury, 2000). Initially, all national assessments, whether used for formative \n",
      "i=27, j=0, k=1\n",
      "a=IMPOSSIBLE\n",
      "g=Trends in International Mathematics and Science Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=National Education Longitudinal Study (NELS:88) tapped children's self-concept and their perceptions of how much control they had over their own lives. See sections 3.3 xxix and 3.4 for more information on these scales and the scores that are available for analysis. The procedures for collecting height and weight data were modified. In the previous rounds of the ECLS-K, height and weight data were collected during the one-on-one direct assessment sessions. In the eighth grade, height and weight data were collected during the group assessment sessions. In most cases the groups were small (in many cases there was a single child). However, in some cases, the assessment sessions had several children participating. In the group assessment sessions, children were measured one at a time at a single height and weight station. The average size of the assessment group was three children and ranged from one to nine children per group. See section 5.5.2 or the ECLS-K Eighth-Grade Methodology Report (NCES 2009-003) (Tourangeau et al. forthcoming) for additional information on the height and weight data collection. In eighth grade, children completed self-administered paper and pencil questionnaires about their school experiences, their activities, their perceptions of themselves, and their weight, diet, and level of exercise. This questionnaire was completed during the group assessment session. The Academic Rating Scale (ARS) was replaced with other items tapping children's classroom behavior and performance. English, mathematics, and science teachers were asked to rate children on their respective domain-relevant skills. Teachers also rated children on their effort (e.g., \"Does this student usually work hard for good grades in your class?\"), behavior (e.g., \"Does this student seem to relate well to other students in your class?\"), and attendance (e.g., \"How often is this student absent from your class?\"). Teachers also were asked to report if they had either spoken to a guidanc\n",
      "i=28, j=1, k=5\n",
      "a=national education longitudinal study\n",
      "g=Early Childhood Longitudinal Study|Education Longitudinal Study|National Education Longitudinal Study\n",
      "\n",
      "\n",
      "is_impossible=0\n",
      "q=what dataset\n",
      "c=el treatments; cost and time savings; reductions in risk and attrition rate; and the reduced need for animal testing beyond the scope of the IMI. The impact IMI projects can generate is primarily related to improvements and amendments within the medicines development process itself, also defined by the expert group as the Bpathway to socio-economic impact^ (105) . However, generating true tangible socio-economic impact will rely heavily on recognition of the benefits arising from the IMI, and subsequent actions by pharmaceutical companies, regulators, payers and policy makers through adoption of novel processes, the implementation of novel or updated guidelines and policies, and so on. biomarkers; 33 preclinical models; 28 clinical models; 11 research databases; and 87 research tools (assays, discovery models) (54) . Several tangible early outcomes were reported. Regarding human capital, of the 257 TI Pharma fellows who had finished their project by the end of 2013, an impressive 98% continued their career in industry, academia and other pharma (e.g., regulatory) and non-pharma organizations. In the area of research activity and knowledge, clear benefit for patients is anticipated through the delivery of a safe morphine dosage regimen for newborns; new clinical protocols for COPD; a vaccine candidate against acute myeloid leukemia; and a disease registry for rare metabolic diseases, which will facilitate development of diagnosis and treatment for this specific group of diseases. A total of 74 follow-up projects could also be identified, which built on the outputs of various TI Pharma projects (Networks & collaboration). Finally, in 2015 TI Pharma merged with another Dutch PPP, resulting in Lygature and continues to successfully manage international PPP projects (current portfolio: >15 projects). One of the most well-described precompetitive biomedical R&D PPPs is the Alzheimer's Disease Neuroimaging Initiative or ADNI (131) (132) (133) (134) (135) (136) (137) . ADNI\n",
      "i=29, j=423, k=425\n",
      "a=adni\n",
      "g=ADNI\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(contexts, questions, truncation=\"only_first\", padding=\"max_length\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "start_logits, end_logits = model(**inputs).values()\n",
    "for i in range(len(start_logits)):    \n",
    "    j = torch.argmax(start_logits[i])  \n",
    "    k = torch.argmax(end_logits[i]) + 1\n",
    "    a = \"IMPOSSIBLE\"\n",
    "    if 0 < j < k:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[i][j:k])\n",
    "        a = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"\\n\\nis_impossible={is_impossible[i]}\\nq={questions[i]}\\nc={contexts[i]}\\ni={i}, j={j}, k={k}\\na={a}\\ng={golds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ce32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
