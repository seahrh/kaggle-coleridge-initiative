{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea608a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adc61814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('input/squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('input/squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8da5e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two â€“ fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaaef132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='pretrained/distilbert-base-cased-distilled-squad', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "pretrained_dir = \"pretrained/distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, model_max_length=512)\n",
    "print(repr(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ea12be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "#val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "train_encodings = tokenizer(train_questions, train_contexts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_questions, val_contexts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4e162c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7476ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "973cc735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"pretrained/distilbert-base-cased-distilled-squad\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "CPU times: user 2.27 s, sys: 46.9 ms, total: 2.31 s\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(pretrained_dir)\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26856d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:17<00:00, 25.87s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=10.4121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:10<00:00, 23.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=4.7230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "max_examples = 3 * 16\n",
    "indices = range(len(train_dataset))\n",
    "indices = random.sample(indices, max_examples)\n",
    "sample_ds = Subset(train_dataset, indices)\n",
    "train_loader = DataLoader(sample_ds, batch_size=16, shuffle=True)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    loss_mean = 0\n",
    "    steps = len(train_loader)\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss_mean += loss / steps\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"epoch={epoch}, loss={loss_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68038b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d94470c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6c15336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"tmp\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": true,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.5.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "CPU times: user 2.03 s, sys: 62.5 ms, total: 2.09 s\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"tmp\")\n",
    "print(repr(model.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b75741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1731,  1242,  3073,  4487,  9044,  3584,  1132,  1907,  1107,\n",
      "         25267,   136,   102,   100, 25267,   113,  3147,  1227,  1112,   185,\n",
      "         25669,  1766,  1732,   118, 11303,  1468,  1105,   185, 25669,  1766,\n",
      "          1732,   118,  3073,  4487,  9044,   118,  1129,  3740,   114,  2790,\n",
      "          1704,   118,  3007,  4220,  1116,   113,   139,  9637,  1942,   117,\n",
      "         15175,  1942,   118,   123,   117,   155,  1186, 27211, 10460,  1161,\n",
      "           117,   161, 22074,   117, 12120,  2050,  2723,  2064,  7340,   117,\n",
      "           161,  2162, 25264,   795,   114,  1111,  6240,  6828, 21300,   113,\n",
      "         21239,  2591,   114,  1105,  6240,  6828, 10617,   113, 21239,  2349,\n",
      "           114,  1114,  1166,  2724,   116,  3073,  4487,  9044,  3584,  1107,\n",
      "          1620,   116,  3483,  1105,  1996,  9455, 19807,  6328,  1206,  5157,\n",
      "         21484,  2271,  6737,   123,   119,   121,  1105,   153,  1183,  1942,\n",
      "          1766,  1732,   119,   102],\n",
      "        [  101,  1327,  1674, 25267,  2194,   136,   102,   100, 25267,   113,\n",
      "          3147,  1227,  1112,   185, 25669,  1766,  1732,   118, 11303,  1468,\n",
      "          1105,   185, 25669,  1766,  1732,   118,  3073,  4487,  9044,   118,\n",
      "          1129,  3740,   114,  2790,  1704,   118,  3007,  4220,  1116,   113,\n",
      "           139,  9637,  1942,   117, 15175,  1942,   118,   123,   117,   155,\n",
      "          1186, 27211, 10460,  1161,   117,   161, 22074,   117, 12120,  2050,\n",
      "          2723,  2064,  7340,   117,   161,  2162, 25264,   795,   114,  1111,\n",
      "          6240,  6828, 21300,   113, 21239,  2591,   114,  1105,  6240,  6828,\n",
      "         10617,   113, 21239,  2349,   114,  1114,  1166,  2724,   116,  3073,\n",
      "          4487,  9044,  3584,  1107,  1620,   116,  3483,  1105,  1996,  9455,\n",
      "         19807,  6328,  1206,  5157, 21484,  2271,  6737,   123,   119,   121,\n",
      "          1105,   153,  1183,  1942,  1766,  1732,   119,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101, 25267,  2790,  9455, 19807,  6328,  1206,  1134,  8297,  1116,\n",
      "           136,   102,   100, 25267,   113,  3147,  1227,  1112,   185, 25669,\n",
      "          1766,  1732,   118, 11303,  1468,  1105,   185, 25669,  1766,  1732,\n",
      "           118,  3073,  4487,  9044,   118,  1129,  3740,   114,  2790,  1704,\n",
      "           118,  3007,  4220,  1116,   113,   139,  9637,  1942,   117, 15175,\n",
      "          1942,   118,   123,   117,   155,  1186, 27211, 10460,  1161,   117,\n",
      "           161, 22074,   117, 12120,  2050,  2723,  2064,  7340,   117,   161,\n",
      "          2162, 25264,   795,   114,  1111,  6240,  6828, 21300,   113, 21239,\n",
      "          2591,   114,  1105,  6240,  6828, 10617,   113, 21239,  2349,   114,\n",
      "          1114,  1166,  2724,   116,  3073,  4487,  9044,  3584,  1107,  1620,\n",
      "           116,  3483,  1105,  1996,  9455, 19807,  6328,  1206,  5157, 21484,\n",
      "          2271,  6737,   123,   119,   121,  1105,   153,  1183,  1942,  1766,\n",
      "          1732,   119,   102,     0],\n",
      "        [  101,  1327,  1110, 25267,  2331,  1227,  1112,   136,   102,   100,\n",
      "         25267,   113,  3147,  1227,  1112,   185, 25669,  1766,  1732,   118,\n",
      "         11303,  1468,  1105,   185, 25669,  1766,  1732,   118,  3073,  4487,\n",
      "          9044,   118,  1129,  3740,   114,  2790,  1704,   118,  3007,  4220,\n",
      "          1116,   113,   139,  9637,  1942,   117, 15175,  1942,   118,   123,\n",
      "           117,   155,  1186, 27211, 10460,  1161,   117,   161, 22074,   117,\n",
      "         12120,  2050,  2723,  2064,  7340,   117,   161,  2162, 25264,   795,\n",
      "           114,  1111,  6240,  6828, 21300,   113, 21239,  2591,   114,  1105,\n",
      "          6240,  6828, 10617,   113, 21239,  2349,   114,  1114,  1166,  2724,\n",
      "           116,  3073,  4487,  9044,  3584,  1107,  1620,   116,  3483,  1105,\n",
      "          1996,  9455, 19807,  6328,  1206,  5157, 21484,  2271,  6737,   123,\n",
      "           119,   121,  1105,   153,  1183,  1942,  1766,  1732,   119,   102,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  2627,  8620, 25267,   136,   102,   100, 25267,   113,  3147,\n",
      "          1227,  1112,   185, 25669,  1766,  1732,   118, 11303,  1468,  1105,\n",
      "           185, 25669,  1766,  1732,   118,  3073,  4487,  9044,   118,  1129,\n",
      "          3740,   114,  2790,  1704,   118,  3007,  4220,  1116,   113,   139,\n",
      "          9637,  1942,   117, 15175,  1942,   118,   123,   117,   155,  1186,\n",
      "         27211, 10460,  1161,   117,   161, 22074,   117, 12120,  2050,  2723,\n",
      "          2064,  7340,   117,   161,  2162, 25264,   795,   114,  1111,  6240,\n",
      "          6828, 21300,   113, 21239,  2591,   114,  1105,  6240,  6828, 10617,\n",
      "           113, 21239,  2349,   114,  1114,  1166,  2724,   116,  3073,  4487,\n",
      "          9044,  3584,  1107,  1620,   116,  3483,  1105,  1996,  9455, 19807,\n",
      "          6328,  1206,  5157, 21484,  2271,  6737,   123,   119,   121,  1105,\n",
      "           153,  1183,  1942,  1766,  1732,   119,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1165,  1108, 25267,  8620,   136,   102,   100, 25267,   113,\n",
      "          3147,  1227,  1112,   185, 25669,  1766,  1732,   118, 11303,  1468,\n",
      "          1105,   185, 25669,  1766,  1732,   118,  3073,  4487,  9044,   118,\n",
      "          1129,  3740,   114,  2790,  1704,   118,  3007,  4220,  1116,   113,\n",
      "           139,  9637,  1942,   117, 15175,  1942,   118,   123,   117,   155,\n",
      "          1186, 27211, 10460,  1161,   117,   161, 22074,   117, 12120,  2050,\n",
      "          2723,  2064,  7340,   117,   161,  2162, 25264,   795,   114,  1111,\n",
      "          6240,  6828, 21300,   113, 21239,  2591,   114,  1105,  6240,  6828,\n",
      "         10617,   113, 21239,  2349,   114,  1114,  1166,  2724,   116,  3073,\n",
      "          4487,  9044,  3584,  1107,  1620,   116,  3483,  1105,  1996,  9455,\n",
      "         19807,  6328,  1206,  5157, 21484,  2271,  6737,   123,   119,   121,\n",
      "          1105,   153,  1183,  1942,  1766,  1732,   119,   102,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"How many pretrained models are available in Transformers?\",\n",
    "    \"What does Transformers provide?\",\n",
    "    \"Transformers provides interoperability between which frameworks?\",\n",
    "    \"What is Transformers previously known as?\",\n",
    "    \"Who invented Transformers?\",\n",
    "    \"when was Transformers invented?\",\n",
    "]\n",
    "contexts = [r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"] * len(questions)\n",
    "enc = tokenizer(questions, contexts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "print(repr(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e4b90af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(outputs)=2, start_logits.size=torch.Size([6, 124])\n",
      "odict_values([tensor([[ -3.6490,  -3.6246,  -6.2458,  -5.6943,  -8.1695,  -7.8217,  -5.5975,\n",
      "          -7.1176,  -5.9592,  -7.1712,  -3.8118,  -4.3419,  -7.1055,  -2.8557,\n",
      "          -3.3907,  -6.6875,  -6.4565,  -7.9877,  -8.9799,  -5.4271,  -8.4327,\n",
      "          -9.4773,  -8.8143,  -9.0330,  -8.1665,  -8.5648,  -8.3357,  -5.4174,\n",
      "          -9.0065,  -9.9294,  -8.7472,  -9.0862,  -7.5517,  -9.0491,  -8.9072,\n",
      "          -9.0021,  -8.3361,  -8.7396,  -6.5901,  -6.0266,  -5.4052,  -7.8211,\n",
      "          -7.3749,  -6.7588,  -7.3100,  -5.9891,  -5.1472,  -8.3681,  -8.1012,\n",
      "          -7.6546,  -6.4833,  -8.7807,  -8.8381,  -7.3546,  -8.1153,  -6.9874,\n",
      "          -8.8220,  -8.7417,  -8.9461,  -8.4285,  -8.0074,  -7.0086,  -8.7146,\n",
      "          -8.1232,  -7.0831,  -9.4768,  -8.7962,  -8.7904,  -8.6294,  -8.2740,\n",
      "          -6.6604,  -8.7491,  -8.2894,  -6.4729,  -6.3459,  -6.4018,  -4.8097,\n",
      "          -7.0090,  -6.5348,  -6.0109,  -4.4547,  -7.0041,  -6.8309,  -7.3532,\n",
      "          -4.2990,  -7.0332,  -6.4126,  -5.6923,  -4.4479,  -6.9656,  -6.1870,\n",
      "          -4.4894,  -0.0222,  -0.4504,  -4.7737,  -5.2444,  -7.4453,  -6.4304,\n",
      "          -5.5427,  -6.9753,  -4.0161,  -6.9804,  -6.5291,  -8.8390,  -6.5687,\n",
      "          -7.4355,  -8.8399,  -8.2037,  -7.2874,  -5.2575,  -7.7649,  -8.5671,\n",
      "          -7.7952,  -5.5643,  -7.8799,  -7.2892,  -8.0818,  -6.4022,  -8.9417,\n",
      "          -9.0611,  -9.9781,  -7.9824,  -7.4335,  -7.1055],\n",
      "        [ -2.1404,   0.2864,  -2.8442,  -2.1065,  -2.4663,  -2.1746,  -5.2081,\n",
      "          -1.4371,  -1.5062,  -3.1971,  -3.8400,  -5.3242,  -5.9292,  -2.0014,\n",
      "          -5.7115,  -6.4235,  -5.6138,  -5.8439,  -4.7259,  -5.9130,  -5.8917,\n",
      "          -1.8494,  -5.8151,  -6.4710,  -5.7876,  -5.8745,  -4.8914,  -6.6117,\n",
      "          -6.4148,  -6.0524,  -5.2024,  -6.0602,  -4.2475,   0.5841,   2.7140,\n",
      "          -3.8083,  -3.3758,  -1.6217,  -3.8380,  -1.8073,  -2.1604,  -5.5629,\n",
      "          -5.7553,  -5.5523,  -3.4787,  -6.4180,  -6.1327,  -5.4551,  -5.8851,\n",
      "          -3.7551,  -6.2705,  -5.8735,  -6.2803,  -6.1690,  -5.4819,  -4.1439,\n",
      "          -6.0144,  -5.7720,  -4.1163,  -6.7045,  -6.3062,  -6.3596,  -6.1738,\n",
      "          -5.9465,  -3.5818,  -6.1647,  -5.9186,  -4.8366,  -4.1784,  -1.1164,\n",
      "           0.3932,  -4.4758,  -4.1917,  -4.0765,  -2.8309,  -5.8016,  -5.1299,\n",
      "          -4.2240,   0.4275,  -4.5486,  -4.1270,  -4.2422,  -2.9947,  -5.8302,\n",
      "          -5.0532,  -3.5252,  -2.9508,  -3.7724,  -5.3513,  -4.1205,  -6.5164,\n",
      "          -6.1446,  -4.7561,  -5.4336,  -4.1168,  -5.8879,  -5.1821,  -6.2649,\n",
      "          -1.5697,  -3.7093,  -5.9334,  -5.2873,  -4.9270,  -3.4833,  -6.2113,\n",
      "          -6.2516,  -6.2718,  -5.4516,  -6.5635,  -6.3208,  -6.7937,  -3.5764,\n",
      "          -6.4882,  -6.7350,  -7.5677,  -6.3502,  -6.3208,  -5.2081,  -9.2141,\n",
      "          -9.2084,  -9.2226,  -9.2055,  -9.2289,  -9.1731],\n",
      "        [ -1.9420,  -1.5793,  -3.8979,  -3.5246,  -5.8785,  -5.4220,  -3.2317,\n",
      "          -2.5348,  -2.4738,  -4.0092,  -2.4089,  -4.9703,  -2.1269,  -1.8489,\n",
      "          -4.1751,  -4.1524,  -5.2956,  -5.9548,  -2.1065,  -5.5261,  -6.2098,\n",
      "          -5.4425,  -5.7073,  -4.9616,  -5.7020,  -5.7917,  -2.1310,  -6.0587,\n",
      "          -6.6420,  -5.7290,  -5.8556,  -4.9726,  -6.6771,  -6.4334,  -6.0637,\n",
      "          -5.5523,  -5.9057,  -4.4696,  -3.6759,  -2.5558,  -5.6824,  -5.0876,\n",
      "          -4.3426,  -5.3308,  -2.6626,  -1.5986,  -5.4284,  -5.0246,  -5.3136,\n",
      "          -3.2964,  -5.6534,  -5.8663,  -4.8929,  -5.7665,  -3.8277,  -5.9613,\n",
      "          -5.8938,  -6.1494,  -5.6633,  -5.8379,  -4.3959,  -5.8111,  -6.0680,\n",
      "          -4.6924,  -6.9802,  -6.3960,  -6.5133,  -6.0535,  -6.3437,  -4.1074,\n",
      "          -6.3434,  -6.0656,  -4.7614,  -4.8451,  -3.8994,  -1.4091,  -4.8893,\n",
      "          -4.3428,  -4.2057,  -2.9090,  -5.0798,  -4.5846,  -4.9238,  -1.3239,\n",
      "          -4.6792,  -4.2946,  -4.1299,  -2.9450,  -5.2696,  -4.6954,  -4.1353,\n",
      "          -2.9746,  -3.1424,  -4.6498,  -4.5981,  -6.5279,  -6.0648,  -4.5393,\n",
      "          -5.2436,  -3.3513,  -5.0435,  -4.2598,  -3.8840,  -0.5412,  -1.3609,\n",
      "          -4.2013,  -3.6383,   0.2617,   4.7638,  -3.1247,  -3.3949,  -2.8399,\n",
      "          -2.0054,  -3.5583,  -2.8392,  -3.9493,  -0.4608,  -4.5440,  -4.9995,\n",
      "          -5.8927,  -4.0698,  -4.1605,  -4.9703,  -8.9056],\n",
      "        [ -1.1767,   0.8519,  -1.4493,   0.4694,  -1.7076,  -2.1325,  -2.7918,\n",
      "          -0.1227,  -3.0361,   1.1404,   1.9363,   1.1318,   2.6095,   1.1378,\n",
      "           0.6534,   8.1906,  -0.5583,  -1.7116,  -1.3859,  -1.3475,  -0.4816,\n",
      "          -1.9944,  -2.1347,   3.4966,  -1.8325,  -2.6051,  -2.1790,  -2.0226,\n",
      "          -1.0297,  -3.2889,  -3.1213,  -2.5940,  -1.2987,  -2.1483,  -2.1273,\n",
      "          -2.4591,  -2.0905,  -4.2760,  -3.7566,  -3.2033,  -3.8370,  -2.5321,\n",
      "          -0.5902,  -4.1627,  -3.8861,  -3.9476,  -1.7909,  -4.2545,  -4.4526,\n",
      "          -3.4814,  -4.1280,  -1.9062,  -4.0380,  -4.0937,  -4.3325,  -4.0225,\n",
      "          -4.0695,  -2.5435,  -4.1379,  -4.1416,  -2.0571,  -4.6470,  -4.2619,\n",
      "          -4.4134,  -3.8931,  -4.0211,  -1.7626,  -4.4359,  -3.7745,  -2.6617,\n",
      "          -3.1884,  -3.4193,  -0.6519,  -3.7133,  -3.2347,  -3.4140,  -2.1027,\n",
      "          -4.1005,  -3.8371,  -4.0525,  -0.2107,  -3.2744,  -2.8035,  -3.0114,\n",
      "          -1.6815,  -3.9765,  -3.6654,  -4.0637,  -3.0671,  -2.9873,  -4.0773,\n",
      "          -3.4836,  -5.1187,  -4.8246,  -3.7538,  -4.5714,  -3.0633,  -4.2814,\n",
      "          -3.7020,  -5.3256,  -3.1666,  -3.5992,  -4.7485,  -4.5238,  -4.1806,\n",
      "          -2.2607,  -4.2700,  -4.4591,  -4.3849,  -3.6104,  -4.9598,  -4.4212,\n",
      "          -4.8641,  -1.3688,  -4.1999,  -4.6964,  -5.3820,  -4.2657,  -4.2017,\n",
      "          -3.0360,  -6.5956,  -6.5695,  -6.6568,  -6.6582],\n",
      "        [ -3.0967,  -2.7810,  -3.2077,  -2.4266,  -2.6503,  -7.3188,  -2.8745,\n",
      "          -2.5961,  -7.1641,  -6.7402,  -9.0098,  -9.9712,  -3.9768,  -8.6586,\n",
      "          -8.9159,  -7.2437,  -8.8506,  -6.5467,  -7.2783,  -9.2425,  -4.9863,\n",
      "          -9.4052,  -9.2807,  -8.0197,  -9.4559,  -7.5967,  -9.2683,  -8.8400,\n",
      "          -9.1899,  -7.8250,  -7.9520,  -6.9108,  -6.7480,  -6.8060,  -8.8439,\n",
      "          -7.9810,  -7.4289,  -8.0770,  -8.5290,  -5.8177,  -9.3705,  -8.1814,\n",
      "          -8.6473,  -6.0917,  -8.7263,  -9.3553,  -7.6069,  -9.1738,  -7.3054,\n",
      "          -9.6913,  -9.3002,  -9.0508,  -8.6960,  -8.6952,  -7.4408,  -8.0528,\n",
      "          -8.6833,  -6.2798,  -9.1321,  -8.3464,  -8.5082,  -8.0119,  -8.6321,\n",
      "          -6.4583,  -9.0713,  -8.4133,  -6.9458,  -7.2775,  -7.8987,  -4.6595,\n",
      "          -6.6671,  -6.2875,  -8.4816,  -5.9335,  -8.1553,  -6.9675,  -8.0977,\n",
      "          -4.4899,  -6.8580,  -6.7010,  -9.2052,  -6.7915,  -9.4132,  -8.6317,\n",
      "          -9.1213,  -7.7198,  -7.4293,  -9.1217,  -9.0480, -10.4604, -10.0901,\n",
      "          -8.1894, -10.5910,  -7.7293,  -9.5126,  -8.4853, -10.3824,  -7.5782,\n",
      "          -8.0627,  -9.2899,  -8.2658,  -7.8332,  -4.5686,  -7.8937,  -8.3576,\n",
      "          -8.0914,  -6.9970,  -8.7022,  -7.2959,  -7.8120,  -4.1345,  -8.3810,\n",
      "          -8.7163,  -9.6577,  -7.7581,  -8.3529,  -7.3189, -12.3913, -12.3055,\n",
      "         -12.3935, -12.4377, -12.4108, -12.3244, -12.5585],\n",
      "        [ -3.3081,  -3.9821,  -6.0917,  -3.9671,  -4.8269,  -3.0457,  -7.3517,\n",
      "          -1.9801,  -3.0908,  -7.5030,  -7.2007, -10.0036, -10.8745,  -5.9124,\n",
      "         -10.3680, -11.1188, -10.1883, -11.0536,  -9.2769,  -9.4742, -11.2617,\n",
      "          -6.8599, -10.9822, -11.4752, -10.6290, -11.3876,  -9.8430, -11.1726,\n",
      "         -11.3034, -11.0217, -10.0090,  -9.3928,  -8.4508,  -8.1845,  -7.8115,\n",
      "         -10.1407,  -9.9075,  -9.0076,  -9.2642,  -9.1927,  -7.2503, -11.0275,\n",
      "         -10.5264, -10.3360,  -8.0820, -10.8658, -11.0998,  -9.6406, -10.5026,\n",
      "          -9.0606, -11.2664, -11.0204, -10.9141, -10.6554, -10.5525,  -9.4542,\n",
      "         -10.4603, -10.8449,  -9.0949, -11.5284, -11.2744, -10.9978, -10.2441,\n",
      "         -10.6718,  -8.6457, -11.1048, -10.0598,  -8.1737,  -8.7340,  -9.4509,\n",
      "          -6.2611,  -8.8375,  -8.0070,  -9.8038,  -7.4490,  -9.5684,  -9.4626,\n",
      "         -10.5581,  -6.3038,  -9.3583,  -8.3343,  -9.6571,  -8.0238, -10.1104,\n",
      "          -9.5674, -10.1721,  -7.8910,  -7.9755, -10.0752, -10.3887, -11.5366,\n",
      "         -11.4432,  -8.9803, -10.9724,  -7.9599, -10.1123,  -9.0995, -11.8648,\n",
      "          -8.5241,  -9.4327, -10.9881, -10.0720,  -8.7804,  -6.1034,  -9.7051,\n",
      "         -10.1251,  -9.6815,  -7.6723,  -9.5512,  -8.2510,  -9.9950,  -7.3797,\n",
      "         -10.9041, -11.2109, -12.0658,  -9.3494,  -9.8369,  -7.3518, -12.9991,\n",
      "         -13.0179, -13.0108, -13.0278, -13.0620, -13.0304]],\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[-1.7404e+00, -4.6167e+00, -2.5475e+00, -5.5287e+00, -4.9156e+00,\n",
      "         -2.9493e+00, -7.3208e-01, -4.7553e+00, -2.8615e+00, -6.3101e+00,\n",
      "         -1.1573e+00, -3.2812e+00, -6.9314e+00, -2.6122e+00, -1.7781e+00,\n",
      "         -5.3654e+00, -6.0608e+00, -6.0658e+00, -7.1108e+00, -7.1819e+00,\n",
      "         -6.7007e+00, -7.1565e+00, -5.5744e+00, -6.7625e+00, -6.7822e+00,\n",
      "         -4.6558e+00, -6.8415e+00, -7.6731e+00, -7.2280e+00, -7.4672e+00,\n",
      "         -4.7909e+00, -6.9897e+00, -6.4936e+00, -5.8140e+00, -4.5913e+00,\n",
      "         -6.4783e+00, -7.3550e+00, -4.7887e+00, -3.1332e+00, -5.3207e+00,\n",
      "         -5.3508e+00, -5.3755e+00, -4.4880e+00, -4.9141e+00, -3.3630e+00,\n",
      "         -5.9979e+00, -6.3103e+00, -6.4117e+00, -4.2632e+00, -4.6894e+00,\n",
      "         -6.6606e+00, -5.6112e+00, -6.0505e+00, -4.2129e+00, -4.8352e+00,\n",
      "         -7.2701e+00, -6.3403e+00, -6.1931e+00, -6.0461e+00, -4.3687e+00,\n",
      "         -5.0645e+00, -6.8441e+00, -4.7330e+00, -4.9485e+00, -6.7322e+00,\n",
      "         -6.7647e+00, -6.0745e+00, -6.3535e+00, -4.4480e+00, -5.0647e+00,\n",
      "         -6.5787e+00, -6.0811e+00, -3.9292e+00, -3.9700e+00, -3.5068e+00,\n",
      "         -6.0294e+00, -5.6373e+00, -4.1561e+00, -3.0082e+00, -5.2196e+00,\n",
      "         -4.8324e+00, -2.9053e+00, -2.9546e+00, -5.4081e+00, -5.5444e+00,\n",
      "         -4.4446e+00, -2.7582e+00, -4.9598e+00, -4.6473e+00, -2.1157e+00,\n",
      "         -2.0051e+00, -5.7265e+00, -4.6410e+00, -1.4724e+00,  8.3041e-01,\n",
      "         -4.3064e+00, -4.1190e+00, -8.7419e-01,  9.3228e-01, -5.8127e+00,\n",
      "         -3.5071e+00, -2.5978e+00, -1.0549e+00, -4.5532e+00, -6.2952e+00,\n",
      "         -6.7718e+00, -6.1026e+00, -3.5703e+00, -6.0548e+00, -6.1295e+00,\n",
      "         -5.8282e+00, -6.3541e+00, -3.6307e+00, -4.6023e+00, -5.1525e+00,\n",
      "         -2.7880e+00, -5.2594e+00, -6.3030e+00, -6.1093e+00, -5.9540e+00,\n",
      "         -6.4859e+00, -2.0615e+00, -2.3185e+00, -6.9314e+00],\n",
      "        [ 7.0956e-03, -1.4986e+00, -3.1594e+00, -1.3941e+00, -2.2631e+00,\n",
      "         -2.2642e+00, -4.8676e+00, -1.7934e+00, -8.2713e-01, -3.6463e+00,\n",
      "         -4.5425e+00, -4.2483e+00, -4.9987e+00, -5.2447e+00, -4.7605e+00,\n",
      "         -4.5952e+00, -2.6091e+00, -4.2568e+00, -4.1603e+00, -1.4385e+00,\n",
      "         -4.4094e+00, -4.9958e+00, -4.5464e+00, -4.3543e+00, -2.4989e+00,\n",
      "         -4.1003e+00, -4.6333e+00, -4.2899e+00, -2.6389e+00, -3.5901e+00,\n",
      "         -4.1315e+00, -1.8941e+00, -1.3730e+00, -3.0606e+00, -2.9480e+00,\n",
      "         -2.9257e+00, -9.4178e-01, -1.9064e+00,  2.8652e+00, -1.9936e+00,\n",
      "         -4.0491e+00, -3.7264e+00, -9.8864e-01, -1.9158e+00, -4.3141e+00,\n",
      "         -3.2162e+00, -2.8058e+00, -1.1463e+00, -1.9923e+00, -4.5434e+00,\n",
      "         -3.8901e+00, -3.2837e+00, -2.9771e+00, -1.1373e+00, -1.6539e+00,\n",
      "         -3.8984e+00, -1.0944e+00, -1.9099e+00, -3.9284e+00, -3.7254e+00,\n",
      "         -3.5817e+00, -3.3577e+00, -1.3799e+00, -2.1948e+00, -3.8068e+00,\n",
      "         -3.1528e+00, -2.3645e-01, -9.1259e-01,  8.8102e-01, -2.9985e+00,\n",
      "         -3.6758e+00, -2.0690e+00,  1.0538e+00, -2.3556e+00, -2.7520e+00,\n",
      "          9.0775e-02,  1.2240e+00, -2.8449e+00, -3.7192e+00, -1.9852e+00,\n",
      "          9.9177e-01, -2.3443e+00, -2.9451e+00, -3.8511e-01,  9.3548e-01,\n",
      "         -3.3826e+00, -4.2822e+00, -3.0660e+00, -2.2729e+00, -4.0554e+00,\n",
      "         -3.5698e+00, -1.6872e+00,  1.5373e-01, -3.7422e+00, -2.5933e+00,\n",
      "         -2.2075e+00,  1.3624e-01, -3.4678e+00, -4.2753e+00, -4.9896e+00,\n",
      "         -4.0802e+00, -1.1827e-01, -3.7807e+00, -4.7388e+00, -4.5262e+00,\n",
      "         -4.3571e+00, -1.9528e+00, -3.3931e+00, -3.7079e+00, -1.4666e+00,\n",
      "         -4.0891e+00, -4.7672e+00, -4.0858e+00, -4.1003e+00, -4.3148e+00,\n",
      "          7.7066e-01,  6.8291e-01, -4.8675e+00, -8.3896e+00, -8.3536e+00,\n",
      "         -8.3158e+00, -8.3623e+00, -8.3591e+00, -8.3178e+00],\n",
      "        [-5.0452e-02, -1.2165e+00, -4.2684e+00, -4.2612e+00, -3.4784e+00,\n",
      "         -1.4570e+00, -4.2067e+00, -3.1439e+00, -2.3806e+00, -2.8293e-01,\n",
      "         -2.0338e+00, -4.2911e+00, -2.0358e+00, -1.2183e+00, -3.8308e+00,\n",
      "         -4.3520e+00, -4.1973e+00, -5.1277e+00, -4.8718e+00, -4.8715e+00,\n",
      "         -4.9475e+00, -3.2471e+00, -4.9316e+00, -4.6673e+00, -3.0975e+00,\n",
      "         -4.6621e+00, -4.6497e+00, -4.4953e+00, -4.4484e+00, -2.2449e+00,\n",
      "         -4.1542e+00, -4.1569e+00, -4.2697e+00, -2.3906e+00, -3.7183e+00,\n",
      "         -3.9852e+00, -1.0847e+00, -1.3011e+00, -3.3256e+00, -3.7847e+00,\n",
      "         -4.1103e+00, -2.5225e+00, -3.4527e+00, -1.6434e+00, -4.7554e+00,\n",
      "         -4.6332e+00, -4.5329e+00, -2.6976e+00, -3.6320e+00, -4.2041e+00,\n",
      "         -3.5151e+00, -3.7838e+00, -2.2830e+00, -3.4934e+00, -4.5899e+00,\n",
      "         -3.9196e+00, -3.7320e+00, -3.6439e+00, -2.5406e+00, -3.6370e+00,\n",
      "         -4.1707e+00, -2.4831e+00, -3.5938e+00, -4.0743e+00, -4.3723e+00,\n",
      "         -3.7526e+00, -3.9689e+00, -1.9640e+00, -3.6139e+00, -4.1298e+00,\n",
      "         -3.8217e+00, -1.0607e+00, -1.2916e+00, -1.9933e+00, -3.9478e+00,\n",
      "         -3.6193e+00, -2.3258e+00, -1.2944e+00, -3.2428e+00, -3.3144e+00,\n",
      "         -1.6868e+00, -1.7503e+00, -3.4979e+00, -3.5206e+00, -2.1997e+00,\n",
      "         -1.3857e-01, -2.9271e+00, -3.1732e+00, -4.7050e-01, -1.5437e-02,\n",
      "         -3.5122e+00, -4.0446e+00, -3.0093e+00, -2.3751e+00, -4.1778e+00,\n",
      "         -4.1338e+00, -2.6944e+00, -1.4965e+00, -4.6778e+00, -2.7928e+00,\n",
      "         -2.3363e+00, -1.0706e+00, -3.7928e+00, -2.2417e+00, -3.4931e+00,\n",
      "         -3.1205e+00, -9.6989e-02, -2.9293e+00, -2.9509e+00, -3.0041e+00,\n",
      "         -3.0341e+00, -2.6080e-01, -1.7215e+00, -2.0060e+00,  2.7190e-01,\n",
      "         -1.3293e+00, -2.8772e+00, -1.9615e+00, -2.2383e+00, -2.5065e+00,\n",
      "          5.2805e+00,  4.3462e+00, -4.2911e+00, -7.8244e+00],\n",
      "        [ 3.9644e-01,  4.0163e-01, -1.8560e+00,  4.7625e-01, -1.6243e+00,\n",
      "         -1.2523e+00, -4.8054e-01, -4.9069e-01, -2.8385e+00,  2.2519e-01,\n",
      "          1.0669e+00, -2.0034e+00, -2.2301e+00, -2.5101e+00, -3.3132e+00,\n",
      "         -2.6244e-01, -1.3169e+00, -7.0192e-01,  1.4575e+00, -4.7742e-01,\n",
      "         -2.7258e-01,  6.4035e+00, -3.3136e-01, -2.0958e+00, -1.7555e+00,\n",
      "         -1.3659e+00,  9.0305e-01, -8.9280e-01, -1.1411e+00, -1.1812e+00,\n",
      "          8.6466e-01, -1.7384e-01, -5.6040e-01,  7.2114e+00,  3.5069e+00,\n",
      "         -1.9722e+00, -2.3876e+00, -2.5065e+00, -1.5410e+00, -1.8810e+00,\n",
      "         -3.2179e-01, -2.6801e+00, -2.4896e+00, -2.8907e+00, -5.0606e-01,\n",
      "         -1.8909e+00, -2.8665e+00, -2.0409e+00, -2.3120e+00, -5.8946e-01,\n",
      "         -1.8294e+00, -2.9835e+00, -2.4370e+00, -2.3820e+00, -2.1187e+00,\n",
      "         -4.1728e-01, -2.0231e+00, -2.9401e+00, -9.6041e-01, -2.0630e+00,\n",
      "         -2.5899e+00, -2.6434e+00, -2.4402e+00, -2.6101e+00, -4.1577e-01,\n",
      "         -1.9641e+00, -2.6414e+00, -2.5132e+00, -6.9375e-02, -1.3979e-01,\n",
      "         -7.5475e-01, -2.9706e+00, -2.3095e+00, -1.7237e+00, -5.7310e-01,\n",
      "         -2.9023e+00, -2.5908e+00, -6.9247e-01, -6.5752e-01, -2.9928e+00,\n",
      "         -2.3668e+00, -1.6146e+00, -2.0175e-01, -2.6001e+00, -2.5148e+00,\n",
      "         -1.8001e-01, -2.0114e-01, -2.6650e+00, -3.0905e+00, -2.4261e+00,\n",
      "         -2.0094e+00, -2.9014e+00, -2.9620e+00, -1.8451e+00, -8.9288e-01,\n",
      "         -3.5601e+00, -2.2020e+00, -1.9739e+00, -9.0245e-01, -3.2287e+00,\n",
      "         -3.0613e+00, -3.2266e+00, -3.1042e+00, -1.4952e+00, -3.5132e+00,\n",
      "         -3.1315e+00, -3.0593e+00, -3.3575e+00, -1.5741e+00, -2.6081e+00,\n",
      "         -3.1150e+00, -1.2184e+00, -3.4129e+00, -3.1165e+00, -2.8555e+00,\n",
      "         -2.9460e+00, -3.2852e+00,  3.4205e-01, -5.6036e-01, -2.8383e+00,\n",
      "         -5.8191e+00, -5.8741e+00, -5.9388e+00, -5.9046e+00],\n",
      "        [-1.3580e+00, -2.6384e+00, -2.5734e+00, -9.1199e-01, -2.1195e+00,\n",
      "         -7.2759e+00, -2.6250e+00, -1.4257e+00, -7.1993e+00, -8.0848e+00,\n",
      "         -8.2863e+00, -9.5625e+00, -7.4243e+00, -7.6792e+00, -7.1004e+00,\n",
      "         -4.1925e+00, -7.1082e+00, -5.9049e+00, -2.9297e+00, -7.9175e+00,\n",
      "         -7.9606e+00, -8.0651e+00, -7.1516e+00, -4.7438e+00, -7.5433e+00,\n",
      "         -7.1688e+00, -6.9613e+00, -5.0903e+00, -6.9264e+00, -7.0939e+00,\n",
      "         -3.5342e+00, -3.2879e+00, -6.1375e+00, -7.0666e+00, -6.7936e+00,\n",
      "         -5.4669e+00, -6.0437e+00, -4.5161e+00, -8.7454e+00, -7.1972e+00,\n",
      "         -7.7144e+00, -4.5326e+00, -6.2846e+00, -7.0736e+00, -6.1379e+00,\n",
      "         -6.8904e+00, -4.4750e+00, -6.6614e+00, -8.2606e+00, -7.4962e+00,\n",
      "         -7.1754e+00, -6.6879e+00, -4.9126e+00, -6.4908e+00, -7.6641e+00,\n",
      "         -4.5575e+00, -6.3482e+00, -6.7848e+00, -6.8313e+00, -6.2226e+00,\n",
      "         -6.5203e+00, -4.3232e+00, -6.4034e+00, -7.0385e+00, -6.8515e+00,\n",
      "         -4.7843e+00, -4.8279e+00, -4.7957e+00, -7.7192e+00, -6.1080e+00,\n",
      "         -4.3151e+00, -3.3365e+00, -8.3048e+00, -6.4775e+00, -4.2196e+00,\n",
      "         -3.6164e+00, -7.1357e+00, -6.5978e+00, -4.5354e+00, -3.6264e+00,\n",
      "         -8.4793e+00, -7.1625e+00, -5.1078e+00, -4.8853e+00, -8.1719e+00,\n",
      "         -8.0891e+00, -6.5018e+00, -6.3973e+00, -8.4595e+00, -7.9366e+00,\n",
      "         -6.3569e+00, -4.8098e+00, -9.5630e+00, -6.7206e+00, -6.5434e+00,\n",
      "         -5.2829e+00, -7.6869e+00, -7.4884e+00, -7.7461e+00, -7.3894e+00,\n",
      "         -4.5217e+00, -7.4163e+00, -6.3894e+00, -6.3877e+00, -7.1261e+00,\n",
      "         -4.6786e+00, -5.6774e+00, -6.6610e+00, -3.4035e+00, -6.5716e+00,\n",
      "         -6.7911e+00, -6.3432e+00, -6.5978e+00, -6.8845e+00, -2.6264e+00,\n",
      "         -4.0865e+00, -7.2759e+00, -1.1799e+01, -1.1720e+01, -1.1748e+01,\n",
      "         -1.1739e+01, -1.1749e+01, -1.1701e+01, -1.1873e+01],\n",
      "        [-1.9005e+00, -4.4612e+00, -6.7577e+00, -3.0359e+00, -3.2773e+00,\n",
      "         -3.3169e+00, -7.4466e+00, -2.3957e+00, -2.0515e+00, -7.7204e+00,\n",
      "         -8.3992e+00, -9.2223e+00, -1.0198e+01, -8.9543e+00, -9.5054e+00,\n",
      "         -9.4872e+00, -7.1404e+00, -9.5360e+00, -8.7779e+00, -5.4448e+00,\n",
      "         -1.0162e+01, -9.8497e+00, -9.9430e+00, -9.5870e+00, -7.3627e+00,\n",
      "         -9.8130e+00, -9.6116e+00, -8.9562e+00, -7.6174e+00, -8.9344e+00,\n",
      "         -9.3782e+00, -5.1527e+00, -5.0044e+00, -8.0112e+00, -8.3611e+00,\n",
      "         -8.1696e+00, -7.3230e+00, -7.7774e+00, -5.7149e+00, -9.6575e+00,\n",
      "         -8.7806e+00, -9.5008e+00, -6.6816e+00, -7.9670e+00, -9.2362e+00,\n",
      "         -8.1448e+00, -8.6251e+00, -6.3826e+00, -7.9977e+00, -1.0113e+01,\n",
      "         -9.1850e+00, -8.9825e+00, -8.5669e+00, -6.7348e+00, -8.3377e+00,\n",
      "         -9.8718e+00, -6.7953e+00, -8.5325e+00, -9.6711e+00, -9.3899e+00,\n",
      "         -9.1550e+00, -9.0144e+00, -6.4731e+00, -8.4200e+00, -9.4409e+00,\n",
      "         -8.9033e+00, -6.3006e+00, -6.0874e+00, -6.3562e+00, -9.4257e+00,\n",
      "         -7.7623e+00, -6.4380e+00, -5.1088e+00, -9.4387e+00, -8.1263e+00,\n",
      "         -5.6321e+00, -5.8627e+00, -9.4849e+00, -8.4226e+00, -7.1036e+00,\n",
      "         -5.4121e+00, -8.9217e+00, -8.4866e+00, -6.0267e+00, -5.9370e+00,\n",
      "         -9.6293e+00, -8.6430e+00, -7.4598e+00, -7.4878e+00, -9.9160e+00,\n",
      "         -9.1739e+00, -7.7047e+00, -5.7366e+00, -1.0373e+01, -7.2581e+00,\n",
      "         -7.1379e+00, -5.8682e+00, -9.2644e+00, -9.0521e+00, -9.5379e+00,\n",
      "         -9.2575e+00, -6.3214e+00, -9.2744e+00, -7.9967e+00, -8.3723e+00,\n",
      "         -9.0946e+00, -6.0874e+00, -6.8334e+00, -7.6747e+00, -3.9694e+00,\n",
      "         -8.3391e+00, -9.1555e+00, -8.7772e+00, -9.1227e+00, -9.2552e+00,\n",
      "         -3.7829e+00, -5.4034e+00, -7.4466e+00, -1.2502e+01, -1.2525e+01,\n",
      "         -1.2500e+01, -1.2557e+01, -1.2557e+01, -1.2552e+01]],\n",
      "       grad_fn=<SqueezeBackward1>)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**enc).values()\n",
    "start_logits, end_logits = outputs\n",
    "print(f\"\"\"len(outputs)={len(outputs)}, start_logits.size={start_logits.size()}\n",
    "{repr(outputs)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "abb89ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pretrained models are available in Transformers?\n",
      "j=92, k=99, a=over 32 + pretrained models\n",
      "What does Transformers provide?\n",
      "j=34, k=39, a=general - purpose architectures\n",
      "Transformers provides interoperability between which frameworks?\n",
      "j=108, k=121, a=TensorFlow 2. 0 and PyTorch\n",
      "What is Transformers previously known as?\n",
      "j=15, k=34, a=pytorch - transformers and pytorch - pretrained - bert\n",
      "Who invented Transformers?\n",
      "j=3, k=4, a=Transformers\n",
      "when was Transformers invented?\n",
      "j=7, k=1, a=\n"
     ]
    }
   ],
   "source": [
    "input_ids = enc[\"input_ids\"]\n",
    "for i in range(len(input_ids)):\n",
    "    j = torch.argmax(start_logits[i])  \n",
    "    k = torch.argmax(end_logits[i]) + 1\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[i][j:k])\n",
    "    a = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"{questions[i]}\\nj={j}, k={k}, a={a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bf294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
